{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EEGNet Sleep Disorder Detection - Final Model Training (All Data)\n",
        "# ============================================================================\n",
        "\n",
        "# Cell 0 — Install packages\n",
        "# ============================================================================\n",
        "\n",
        "# Install required packages (run once)\n",
        "%pip install -q mne PyWavelets scikit-learn seaborn  # EEG processing and ML libraries\n",
        "%pip install -q imbalanced-learn  # SMOTE balancing\n",
        "%pip install -q azureml-core azure-ai-ml azure-identity  # Azure ML SDK\n",
        "\n",
        "print(\"✅ Packages installed (if no errors above).\")  # confirm installation\n",
        "\n",
        "\n",
        "# Cell 1 — Load libraries + set seeds\n",
        "# ============================================================================\n",
        "\n",
        "import os  # file and folder operations\n",
        "import random  # Python random number generator\n",
        "import numpy as np  # numerical arrays and math operations\n",
        "\n",
        "import mne  # EEG reading and processing library\n",
        "import pywt  # wavelet transforms\n",
        "from sklearn.decomposition import FastICA  # Independent Component Analysis\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score  # evaluation metrics\n",
        "from sklearn.model_selection import StratifiedKFold  # cross-validation (not used in final)\n",
        "\n",
        "from collections import defaultdict  # dictionary with default values\n",
        "from typing import Optional, Union, Sequence, Dict, Tuple, List  # type hints\n",
        "\n",
        "import matplotlib.pyplot as plt  # plotting library\n",
        "import seaborn as sns  # statistical visualization\n",
        "\n",
        "import tensorflow as tf  # deep learning framework\n",
        "from tensorflow.keras.models import Model  # Keras model class\n",
        "from tensorflow.keras.layers import (  # neural network layers\n",
        "    Conv2D, AveragePooling2D, Flatten, Dense, Dropout, BatchNormalization,\n",
        "    Input, DepthwiseConv2D, SeparableConv2D, Activation\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam  # Adam optimizer\n",
        "from tensorflow.keras.losses import BinaryCrossentropy  # binary classification loss\n",
        "from tensorflow.keras.metrics import BinaryAccuracy  # binary accuracy metric\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger  # training callbacks\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler  # class balancing methods\n",
        "\n",
        "# Try enabling interactive logging (optional)\n",
        "try:\n",
        "    tf.keras.utils.enable_interactive_logging()  # enable better TensorFlow logs if available\n",
        "except Exception:\n",
        "    pass  # ignore if not supported in this environment\n",
        "\n",
        "# Set random seeds for reproducibility across all libraries\n",
        "random.seed(42)  # set Python random seed\n",
        "np.random.seed(42)  # set NumPy random seed\n",
        "tf.random.set_seed(42)  # set TensorFlow random seed\n",
        "\n",
        "print(\"✅ Imports done + seeds set.\")  # confirm imports complete\n",
        "\n",
        "\n",
        "# Cell 2 — Helper: load EEG (.set) with labels\n",
        "# ============================================================================\n",
        "\n",
        "def load_eeg_data_with_target(  # function to load EEG files\n",
        "    folder_path: str,  # path to folder containing .set files\n",
        "    session_name: str,  # session identifier (ses-1 or ses-2)\n",
        "    max_samples: int = 118000,  # maximum samples to keep from start\n",
        "    discard_samples: int = 10000  # samples to discard from beginning\n",
        "):\n",
        "    \"\"\"Load EEG data from .set files and assign labels based on session_name.\"\"\"\n",
        "    \n",
        "    eeg_files = [f for f in os.listdir(folder_path) if f.endswith('.set')]  # find all .set files\n",
        "    data_list = []  # list to store EEG data arrays\n",
        "    targets = []  # list to store class labels\n",
        "    sfreq_list = []  # list to store sampling frequencies\n",
        "\n",
        "    for eeg_file in eeg_files:  # loop through each EEG file\n",
        "        file_path = os.path.join(folder_path, eeg_file)  # create full file path\n",
        "\n",
        "        raw = mne.io.read_raw_eeglab(file_path, preload=True, verbose=False)  # load EEG file\n",
        "        data = raw.get_data().astype(np.float32)  # extract data as float32 (Channels x Time)\n",
        "        sfreq = float(raw.info[\"sfreq\"])  # get sampling frequency\n",
        "\n",
        "        if data.shape[1] > max_samples:  # if recording is too long\n",
        "            data = data[:, :max_samples]  # trim to maximum length\n",
        "\n",
        "        if data.shape[1] > discard_samples:  # if enough samples to discard\n",
        "            data = data[:, discard_samples:]  # remove initial samples (artifact prone)\n",
        "        else:\n",
        "            print(f\"⚠️ Not enough samples to discard in {eeg_file}, skipping.\")  # warn about short file\n",
        "            continue  # skip this file\n",
        "\n",
        "        data_list.append(data)  # add data to list\n",
        "        sfreq_list.append(sfreq)  # add sampling frequency\n",
        "\n",
        "        if session_name == 'ses-1':  # if session 1\n",
        "            targets.append(0)  # assign class 0 label\n",
        "        elif session_name == 'ses-2':  # if session 2\n",
        "            targets.append(1)  # assign class 1 label\n",
        "        else:\n",
        "            print(f\"⚠️ Unknown session name: {session_name}\")  # warn about unknown session\n",
        "\n",
        "    return data_list, targets, sfreq_list  # return loaded data, labels, and frequencies\n",
        "\n",
        "\n",
        "# Cell 3 — Leakage-safe preprocessing classes\n",
        "# ============================================================================\n",
        "\n",
        "def _names_from_index_mapping(  # helper function for channel naming\n",
        "    n_channels: int,  # number of EEG channels\n",
        "    index_to_name: Optional[Dict[int, str]]  # optional mapping of index to name\n",
        ") -> List[str]:\n",
        "    \"\"\"Map numeric channel indices to names or auto-name.\"\"\"\n",
        "    \n",
        "    if index_to_name is None:  # if no mapping provided\n",
        "        return [f\"EEG{i+1}\" for i in range(n_channels)]  # create automatic names EEG1, EEG2, etc.\n",
        "\n",
        "    keys = list(index_to_name.keys())  # get all mapping keys\n",
        "    is_zero_based = (0 in keys) and (1 not in keys)  # detect if mapping is 0-indexed\n",
        "\n",
        "    names = []  # list to store channel names\n",
        "    for i in range(n_channels):  # for each channel\n",
        "        key = i if is_zero_based else (i + 1)  # determine appropriate key\n",
        "        names.append(index_to_name.get(key, f\"EEG{i+1}\"))  # get mapped name or fallback\n",
        "    return names  # return list of channel names\n",
        "\n",
        "\n",
        "def _make_raw(  # helper function to create MNE Raw object\n",
        "    eeg: np.ndarray,  # EEG data array\n",
        "    sfreq: float,  # sampling frequency\n",
        "    ch_names: List[str],  # channel names\n",
        "    use_standard_1020: bool = True  # whether to apply 10-20 montage\n",
        ") -> Tuple[mne.io.Raw, bool]:\n",
        "    \"\"\"Create MNE Raw object and optionally apply 10-20 montage.\"\"\"\n",
        "    \n",
        "    # Determine channel types (EOG or EEG based on name)\n",
        "    ch_types = ['eog' if str(n).upper().startswith(\"EOG\") else 'eeg' for n in ch_names]\n",
        "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)  # create info structure\n",
        "    raw = mne.io.RawArray(eeg.astype(np.float32, copy=False), info, verbose=False)  # create Raw object\n",
        "\n",
        "    montage_applied = False  # flag to track montage application\n",
        "\n",
        "    if use_standard_1020:  # if montage should be applied\n",
        "        try:\n",
        "            mont = mne.channels.make_standard_montage(\"standard_1020\")  # load standard 10-20 montage\n",
        "            raw.set_montage(mont, match_case=False, on_missing=\"ignore\")  # apply montage to channels\n",
        "            montage_applied = True  # mark as successfully applied\n",
        "        except Exception:\n",
        "            montage_applied = False  # mark as failed\n",
        "\n",
        "    return raw, montage_applied  # return Raw object and montage status\n",
        "\n",
        "\n",
        "class WaveletICA:\n",
        "    \"\"\"Wavelet-enhanced ICA (wICA) for artifact removal.\"\"\"\n",
        "    \n",
        "    def __init__(self, wavelet=\"db4\", level=3, n_components=10, random_state=42):\n",
        "        self.wavelet = wavelet  # wavelet type (Daubechies 4)\n",
        "        self.level = level  # decomposition level\n",
        "        self.n_components = n_components  # number of ICA components\n",
        "        self.random_state = random_state  # random seed\n",
        "        self.ica_: Optional[FastICA] = None  # ICA model (fitted later)\n",
        "        self._n_ch: Optional[int] = None  # number of channels\n",
        "\n",
        "    def fit(self, X: np.ndarray):  # fit wICA on training data\n",
        "        C = X.shape[0]  # get number of channels\n",
        "        self._n_ch = C  # store channel count\n",
        "\n",
        "        coeffs = pywt.wavedec(X, wavelet=self.wavelet, level=self.level, axis=1)  # discrete wavelet transform\n",
        "        A = coeffs[0]  # extract approximation coefficients\n",
        "\n",
        "        k = int(min(self.n_components, C))  # limit components to number of channels\n",
        "        self.ica_ = FastICA(n_components=k, random_state=self.random_state)  # create ICA model\n",
        "\n",
        "        S = self.ica_.fit_transform(A.T)  # fit ICA and get independent components\n",
        "        A_denoised = self.ica_.inverse_transform(S).T  # reconstruct denoised approximation\n",
        "\n",
        "        coeffs[0] = A_denoised  # replace original approximation with denoised version\n",
        "        _ = pywt.waverec(coeffs, wavelet=self.wavelet, axis=1)  # sanity check reconstruction\n",
        "\n",
        "        return self  # return self for method chaining\n",
        "\n",
        "    def transform(self, X: np.ndarray) -> np.ndarray:  # apply fitted wICA to new data\n",
        "        assert self.ica_ is not None, \"WaveletICA not fitted yet.\"  # ensure model is fitted\n",
        "\n",
        "        coeffs = pywt.wavedec(X, wavelet=self.wavelet, level=self.level, axis=1)  # wavelet decomposition\n",
        "        A = coeffs[0]  # get approximation coefficients\n",
        "\n",
        "        S = self.ica_.transform(A.T)  # project onto ICA components\n",
        "        A_denoised = self.ica_.inverse_transform(S).T  # reconstruct denoised data\n",
        "\n",
        "        coeffs[0] = A_denoised  # replace approximation\n",
        "        Y = pywt.waverec(coeffs, wavelet=self.wavelet, axis=1)  # inverse wavelet transform\n",
        "\n",
        "        if Y.shape[1] < X.shape[1]:  # if reconstructed signal is shorter\n",
        "            pad_width = X.shape[1] - Y.shape[1]  # calculate padding needed\n",
        "            Y = np.pad(Y, ((0, 0), (0, pad_width)), mode=\"constant\")  # pad with zeros\n",
        "        elif Y.shape[1] > X.shape[1]:  # if reconstructed signal is longer\n",
        "            Y = Y[:, :X.shape[1]]  # trim to original length\n",
        "\n",
        "        return Y.astype(np.float32, copy=False)  # return as float32\n",
        "\n",
        "\n",
        "class EEGPreprocessor:\n",
        "    \"\"\"Leakage-safe preprocessing pipeline for EEG data.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        index_to_name: Optional[Dict[int, str]] = None,  # channel name mapping\n",
        "        use_standard_1020: bool = True,  # apply 10-20 montage\n",
        "        resample_to: Optional[float] = None,  # target sampling rate\n",
        "        notch_freqs: Union[None, float, Sequence[float]] = 50.0,  # notch filter frequencies\n",
        "        highpass: Optional[float] = 0.05,  # highpass filter cutoff\n",
        "        bad_point_z: float = 6.0,  # z-score threshold for bad points\n",
        "        bad_channel_z: float = 5.0,  # z-score threshold for bad channels\n",
        "        interpolate_bad_channels: bool = False,  # whether to interpolate bad channels\n",
        "        car: bool = True,  # common average reference\n",
        "        use_wica: bool = True,  # use wavelet ICA\n",
        "        wica_components: int = 10,  # number of wICA components\n",
        "        wica_wavelet: str = \"db4\",  # wICA wavelet type\n",
        "        wica_level: int = 3,  # wICA decomposition level\n",
        "        wica_random_state: int = 42  # wICA random seed\n",
        "    ):\n",
        "        self.index_to_name = index_to_name\n",
        "        self.use_standard_1020 = use_standard_1020\n",
        "        self.resample_to = resample_to\n",
        "        self.notch_freqs = notch_freqs\n",
        "        self.highpass = highpass\n",
        "        self.bad_point_z = bad_point_z\n",
        "        self.bad_channel_z = bad_channel_z\n",
        "        self.interpolate_bad_channels = interpolate_bad_channels\n",
        "        self.car = car\n",
        "        self.use_wica = use_wica\n",
        "\n",
        "        self._sfreq_out: Optional[float] = None  # output sampling frequency\n",
        "        self._train_mu: Optional[np.ndarray] = None  # training mean per channel\n",
        "        self._train_sd: Optional[np.ndarray] = None  # training std per channel\n",
        "        self._robust_med: Optional[float] = None  # robust median for bad channel detection\n",
        "        self._robust_mad: Optional[float] = None  # robust MAD for bad channel detection\n",
        "        self._train_eeg_names: Optional[List[str]] = None  # EEG channel names\n",
        "\n",
        "        self._wica = WaveletICA(  # create wICA object\n",
        "            wavelet=wica_wavelet,\n",
        "            level=wica_level,\n",
        "            n_components=wica_components,\n",
        "            random_state=wica_random_state\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def sfreq_out(self) -> float:  # property to get output sampling frequency\n",
        "        assert self._sfreq_out is not None, \"Preprocessor not run yet.\"  # ensure preprocessor has run\n",
        "        return float(self._sfreq_out)\n",
        "\n",
        "    def _filter_and_reference(self, raw: mne.io.Raw):  # apply filtering and referencing\n",
        "        if self.resample_to is not None and float(self.resample_to) != float(raw.info[\"sfreq\"]):\n",
        "            raw.resample(self.resample_to, npad=\"auto\")  # resample to target frequency\n",
        "\n",
        "        self._sfreq_out = float(raw.info[\"sfreq\"])  # store output sampling frequency\n",
        "\n",
        "        if self.notch_freqs is not None:\n",
        "            raw.notch_filter(freqs=self.notch_freqs, verbose=False)  # apply notch filter\n",
        "\n",
        "        if self.highpass is not None:\n",
        "            raw.filter(l_freq=self.highpass, h_freq=None, verbose=False)  # apply highpass filter\n",
        "\n",
        "        if self.car:\n",
        "            raw.set_eeg_reference(\"average\", projection=True)  # set common average reference\n",
        "            raw.apply_proj()  # apply projection\n",
        "\n",
        "    def _repair_transients_with_train_stats(self, raw: mne.io.Raw):  # repair transient artifacts\n",
        "        X = raw.get_data()  # get data array\n",
        "        mu = self._train_mu  # get training mean\n",
        "        sd = self._train_sd  # get training std\n",
        "        assert mu is not None and sd is not None, \"Training stats not set.\"  # ensure stats exist\n",
        "\n",
        "        hi = mu + self.bad_point_z * sd  # upper threshold\n",
        "        lo = mu - self.bad_point_z * sd  # lower threshold\n",
        "        mask = (X > hi) | (X < lo)  # identify bad points\n",
        "\n",
        "        if np.any(mask):  # if any bad points found\n",
        "            X_fixed = X.copy()  # copy data\n",
        "            t = np.arange(X.shape[1], dtype=float)  # time indices\n",
        "            for ch in range(X.shape[0]):  # for each channel\n",
        "                m = mask[ch]  # get mask for this channel\n",
        "                if m.any():  # if any bad points in channel\n",
        "                    good = ~m  # identify good points\n",
        "                    if good.sum() >= 2:  # if at least 2 good points\n",
        "                        X_fixed[ch, m] = np.interp(t[m], t[good], X_fixed[ch, good])  # interpolate\n",
        "            raw._data = X_fixed  # update raw data\n",
        "\n",
        "    def _interpolate_bad_channels_with_train_calibration(self, raw: mne.io.Raw):  # interpolate bad channels\n",
        "        if not self.interpolate_bad_channels:\n",
        "            return  # skip if disabled\n",
        "\n",
        "        picks = mne.pick_types(raw.info, eeg=True)  # get EEG channel indices\n",
        "        if len(picks) == 0:\n",
        "            return  # skip if no EEG channels\n",
        "\n",
        "        X = raw.get_data(picks=picks)  # get EEG data\n",
        "        ch_std = X.std(axis=1)  # compute std per channel\n",
        "\n",
        "        med = self._robust_med  # get robust median\n",
        "        mad = self._robust_mad  # get robust MAD\n",
        "        if med is None or mad is None or mad == 0:\n",
        "            return  # skip if stats not available\n",
        "\n",
        "        z = 0.6745 * (ch_std - med) / mad  # compute z-scores\n",
        "        eeg_names = mne.pick_info(raw.info, picks).ch_names  # get EEG channel names\n",
        "        bads = [eeg_names[i] for i in np.where(np.abs(z) > self.bad_channel_z)[0]]  # identify bad channels\n",
        "\n",
        "        raw.info[\"bads\"] = bads  # mark bad channels\n",
        "        if bads:\n",
        "            raw.interpolate_bads(reset_bads=True, verbose=False)  # interpolate bad channels\n",
        "\n",
        "    def fit(self, X_train: np.ndarray, sfreq: float):  # fit preprocessor on training data\n",
        "        C = X_train.shape[0]  # get number of channels\n",
        "        ch_names = _names_from_index_mapping(C, self.index_to_name)  # get channel names\n",
        "\n",
        "        raw_train, montage_applied = _make_raw(X_train, sfreq, ch_names, self.use_standard_1020)  # create Raw\n",
        "        self._filter_and_reference(raw_train)  # apply filtering\n",
        "\n",
        "        Xt = raw_train.get_data()  # get processed data\n",
        "        self._train_mu = Xt.mean(axis=1, keepdims=True)  # compute training mean\n",
        "        self._train_sd = Xt.std(axis=1, keepdims=True) + 1e-12  # compute training std\n",
        "\n",
        "        if montage_applied:  # if montage was applied\n",
        "            picks_eeg = mne.pick_types(raw_train.info, eeg=True)  # get EEG channels\n",
        "            if len(picks_eeg):\n",
        "                X_eeg = Xt[picks_eeg]  # get EEG data\n",
        "                ch_std = X_eeg.std(axis=1)  # compute std per channel\n",
        "                med = np.median(ch_std)  # compute median\n",
        "                mad = np.median(np.abs(ch_std - med)) + 1e-12  # compute MAD\n",
        "                self._robust_med = float(med)  # store median\n",
        "                self._robust_mad = float(mad)  # store MAD\n",
        "                self._train_eeg_names = mne.pick_info(raw_train.info, picks_eeg).ch_names  # store names\n",
        "            else:\n",
        "                self._robust_med = None\n",
        "                self._robust_mad = None\n",
        "        else:\n",
        "            self._robust_med = None\n",
        "            self._robust_mad = None\n",
        "\n",
        "        self._repair_transients_with_train_stats(raw_train)  # repair artifacts\n",
        "        Xt = raw_train.get_data()  # get repaired data\n",
        "\n",
        "        if montage_applied and self.interpolate_bad_channels:\n",
        "            self._interpolate_bad_channels_with_train_calibration(raw_train)  # interpolate bad channels\n",
        "            Xt = raw_train.get_data()  # get interpolated data\n",
        "\n",
        "        if self.use_wica:\n",
        "            self._wica.fit(Xt)  # fit wICA\n",
        "\n",
        "        return self  # return self\n",
        "\n",
        "    def transform(self, X: np.ndarray, sfreq: float) -> Tuple[np.ndarray, float]:  # transform new data\n",
        "        C = X.shape[0]  # get number of channels\n",
        "        ch_names = _names_from_index_mapping(C, self.index_to_name)  # get channel names\n",
        "\n",
        "        raw, montage_applied = _make_raw(X, sfreq, ch_names, self.use_standard_1020)  # create Raw\n",
        "        self._filter_and_reference(raw)  # apply filtering\n",
        "        self._repair_transients_with_train_stats(raw)  # repair artifacts\n",
        "\n",
        "        if montage_applied and self.interpolate_bad_channels:\n",
        "            self._interpolate_bad_channels_with_train_calibration(raw)  # interpolate bad channels\n",
        "\n",
        "        Xf = raw.get_data()  # get processed data\n",
        "        if self.use_wica:\n",
        "            Xf = self._wica.transform(Xf)  # apply wICA\n",
        "\n",
        "        return Xf.astype(np.float32, copy=False), self.sfreq_out  # return processed data and frequency\n",
        "\n",
        "    def fit_transform(self, X_train: np.ndarray, sfreq: float) -> Tuple[np.ndarray, float]:  # fit and transform\n",
        "        self.fit(X_train, sfreq)  # fit on training data\n",
        "        X_clean, fs_out = self.transform(X_train, sfreq)  # transform training data\n",
        "        return X_clean, fs_out  # return cleaned data\n",
        "\n",
        "print(\"✅ Preprocessing classes loaded.\")  # confirm loading complete\n",
        "\n",
        "\n",
        "# Cell 4 — Dataset selection + Azure download + load raw EEG\n",
        "# ============================================================================\n",
        "\n",
        "from azureml.core import Workspace, Datastore  # Azure ML workspace and datastore\n",
        "\n",
        "print(\"[STEP 4] Connecting to Azure ML workspace...\")  # log connection start\n",
        "\n",
        "# Azure workspace credentials (replace with your values)\n",
        "subscription_id = \"eccc04ba-d8b0-4f70-864a-b4a6753bfc72\"  # Azure subscription ID\n",
        "resource_group  = \"somnasnest\"  # Azure resource group name\n",
        "workspace_name  = \"SomnasNest\"  # Azure ML workspace name\n",
        "\n",
        "# Connect to Azure ML workspace\n",
        "ws = Workspace(\n",
        "    subscription_id=subscription_id,\n",
        "    resource_group=resource_group,\n",
        "    workspace_name=workspace_name\n",
        ")\n",
        "\n",
        "print(\"[STEP 4] Getting datastore 'workspaceblobstore'...\")  # log datastore access\n",
        "datastore = Datastore.get(ws, \"workspaceblobstore\")  # get default blob storage\n",
        "\n",
        "remote_prefix = \"UI/2025-12-11_033542_UTC/New Dataset\"  # path in Azure blob storage\n",
        "local_root = \"./azureml_eeg_data\"  # local download directory\n",
        "os.makedirs(local_root, exist_ok=True)  # create directory if it doesn't exist\n",
        "\n",
        "print(f\"[STEP 4] Downloading datastore prefix: {remote_prefix}\")  # log download start\n",
        "n_files = datastore.download(  # download files from Azure\n",
        "    target_path=local_root,  # local destination\n",
        "    prefix=remote_prefix,  # remote path prefix\n",
        "    overwrite=False,  # don't overwrite existing files\n",
        "    show_progress=True,  # show progress bar\n",
        ")\n",
        "print(f\"[STEP 4] Downloaded {n_files} file(s).\")  # log download complete\n",
        "\n",
        "base_path = os.path.join(local_root, *remote_prefix.split(\"/\"))  # construct local base path\n",
        "print(f\"[STEP 4] base_path = {base_path}\")  # log base path\n",
        "\n",
        "if not os.path.isdir(base_path):  # verify path exists\n",
        "    raise RuntimeError(f\"[STEP 4] base_path does not exist: {base_path}\")  # raise error if missing\n",
        "\n",
        "print(\"[STEP 4] Listing base_path contents:\")  # log directory listing\n",
        "print(os.listdir(base_path)[:10], \"...\")  # show first 10 items\n",
        "\n",
        "# ---------------- Dataset Selection Rule ----------------\n",
        "# Rule: sub-01..sub-59 use ses-1 (class 0), sub-60..sub-71 use ses-2 (class 1)\n",
        "\n",
        "sub_ses1 = [f\"sub-{i:02d}\" for i in range(1, 60)]  # subjects 01-59 for session 1\n",
        "sub_ses2 = [f\"sub-{i:02d}\" for i in range(60, 72)]  # subjects 60-71 for session 2\n",
        "subjects_used = sub_ses1 + sub_ses2  # combine all subjects\n",
        "\n",
        "print(\"\\n[STEP 4] Dataset rule summary:\")  # log dataset summary\n",
        "print(f\"  ses-1 subjects count: {len(sub_ses1)} (sub-01..sub-59)\")  # count session 1\n",
        "print(f\"  ses-2 subjects count: {len(sub_ses2)} (sub-60..sub-71)\")  # count session 2\n",
        "print(f\"  total subjects used : {len(subjects_used)}\")  # total subjects\n",
        "\n",
        "# Storage for raw EEG trials\n",
        "raw_list = []  # list to store EEG data arrays\n",
        "targets = []  # list to store class labels\n",
        "sfreqs = []  # list to store sampling frequencies\n",
        "subject_ids = []  # list to store subject IDs\n",
        "\n",
        "print(\"\\n[STEP 4] Loading raw EEG data according to rule...\")  # log loading start\n",
        "\n",
        "# Load session 1 data for subjects 01-59\n",
        "for sub in sub_ses1:  # loop through session 1 subjects\n",
        "    session = \"ses-1\"  # session identifier\n",
        "    path = os.path.join(base_path, sub, session)  # construct folder path\n",
        "    if not os.path.isdir(path):  # check if folder exists\n",
        "        print(f\"⚠️ Missing folder: {path}, skipping.\")  # warn and skip\n",
        "        continue\n",
        "    data_list, t_list, sf_list = load_eeg_data_with_target(path, session)  # load EEG data\n",
        "    print(f\"  Loaded {len(data_list)} trial(s) from {sub}/{session}\")  # log trial count\n",
        "    for data, t, sf in zip(data_list, t_list, sf_list):  # process each trial\n",
        "        raw_list.append(data.astype(np.float32, copy=False))  # store EEG data\n",
        "        targets.append(int(t))  # store label\n",
        "        sfreqs.append(float(sf))  # store sampling frequency\n",
        "        subject_ids.append(sub)  # store subject ID\n",
        "\n",
        "# Load session 2 data for subjects 60-71\n",
        "for sub in sub_ses2:  # loop through session 2 subjects\n",
        "    session = \"ses-2\"  # session identifier\n",
        "    path = os.path.join(base_path, sub, session)  # construct folder path\n",
        "    if not os.path.isdir(path):  # check if folder exists\n",
        "        print(f\"⚠️ Missing folder: {path}, skipping.\")  # warn and skip\n",
        "        continue\n",
        "    data_list, t_list, sf_list = load_eeg_data_with_target(path, session)  # load EEG data\n",
        "    print(f\"  Loaded {len(data_list)} trial(s) from {sub}/{session}\")  # log trial count\n",
        "    for data, t, sf in zip(data_list, t_list, sf_list):  # process each trial\n",
        "        raw_list.append(data.astype(np.float32, copy=False))  # store EEG data\n",
        "        targets.append(int(t))  # store label\n",
        "        sfreqs.append(float(sf))  # store sampling frequency\n",
        "        subject_ids.append(sub)  # store subject ID\n",
        "\n",
        "targets = np.array(targets, dtype=np.int32)  # convert labels to numpy array\n",
        "subject_ids = np.array(subject_ids)  # convert subject IDs to numpy array\n",
        "\n",
        "print(\"\\n[STEP 4] Finished loading raw trials.\")  # log loading complete\n",
        "print(\"  Total trials loaded:\", len(raw_list))  # total trial count\n",
        "print(\"  Targets shape:\", targets.shape)  # labels array shape\n",
        "print(\"  Unique labels + counts:\", np.unique(targets, return_counts=True))  # class distribution\n",
        "\n",
        "# Verify sampling frequencies are consistent\n",
        "sfreqs = np.array(sfreqs, dtype=np.float32)  # convert to numpy array\n",
        "if len(sfreqs) == 0:  # check if any data loaded\n",
        "    raise RuntimeError(\"No EEG data found. Check paths / dataset.\")  # error if no data\n",
        "\n",
        "fs = float(sfreqs[0])  # take first sampling frequency as reference\n",
        "if not np.allclose(sfreqs, fs):  # check if all frequencies match\n",
        "    print(\"⚠️ Warning: Not all sampling frequencies are identical!\")  # warn about mismatch\n",
        "print(f\"[STEP 4] Using fs={fs} Hz\")  # log sampling frequency\n",
        "\n",
        "\n",
        "# Cell 5 — Preprocess all trials\n",
        "# ============================================================================\n",
        "\n",
        "CHANNEL_MAP = None  # no custom channel mapping (use default)\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "pre = EEGPreprocessor(\n",
        "    index_to_name=CHANNEL_MAP,  # channel name mapping\n",
        "    use_standard_1020=True,  # apply 10-20 montage\n",
        "    resample_to=None,  # no resampling\n",
        "    notch_freqs=[50.0, 100.0, 150.0],  # notch filter at 50, 100, 150 Hz\n",
        "    highpass=0.05,  # highpass filter at 0.05 Hz\n",
        "    bad_point_z=6.0,  # z-threshold for artifact detection\n",
        "    bad_channel_z=5.0,  # z-threshold for bad channel detection\n",
        "    interpolate_bad_channels=False,  # disable channel interpolation\n",
        "    car=True,  # enable common average reference\n",
        "    use_wica=True,  # enable wavelet ICA\n",
        "    wica_components=10,  # 10 ICA components\n",
        "    wica_wavelet=\"db4\",  # Daubechies 4 wavelet\n",
        "    wica_level=3,  # 3-level decomposition\n",
        "    wica_random_state=42,  # random seed for reproducibility\n",
        ")\n",
        "\n",
        "print(\"[STEP 5] Fitting EEGPreprocessor on a subset of loaded trials...\")  # log fitting start\n",
        "\n",
        "max_calib_trials = min(10, len(raw_list))  # use up to 10 trials for calibration\n",
        "if max_calib_trials == 0:  # check if any data available\n",
        "    raise RuntimeError(\"No data to fit EEGPreprocessor.\")  # error if no data\n",
        "\n",
        "calib_trials = raw_list[:max_calib_trials]  # select first N trials for calibration\n",
        "X_calib = np.concatenate(calib_trials, axis=1).astype(np.float32, copy=False)  # concatenate along time axis\n",
        "\n",
        "X_calib_clean, fs_out = pre.fit_transform(X_calib, fs)  # fit and transform calibration data\n",
        "print(f\"[STEP 5] Preprocessor fitted. Output fs: {fs_out} Hz\")  # log output frequency\n",
        "\n",
        "data_clean = []  # list to store preprocessed trials\n",
        "for i, segment in enumerate(raw_list):  # loop through all raw trials\n",
        "    X_clean, _ = pre.transform(segment, fs)  # apply preprocessing to trial\n",
        "    data_clean.append(X_clean.astype(np.float32, copy=False))  # store cleaned trial\n",
        "    if (i + 1) % 20 == 0:  # log progress every 20 trials\n",
        "        print(f\"[STEP 5] Preprocessed {i+1}/{len(raw_list)} trials\")\n",
        "\n",
        "data_clean = np.array(data_clean, dtype=np.float32)  # convert list to numpy array\n",
        "\n",
        "print(\"[STEP 5] Done preprocessing all trials.\")  # log completion\n",
        "print(\"  data_clean shape:\", data_clean.shape)  # log shape\n",
        "print(\"  targets shape   :\", targets.shape)  # log targets shape\n",
        "\n",
        "\n",
        "# Cell 6 — Augmentation (split trials into segments)\n",
        "# ============================================================================\n",
        "\n",
        "def augment_data(data: np.ndarray, target: int, segment_size: int = 100):\n",
        "    \"\"\"Split (C,T) trial into segments (C,segment_size) with same label.\"\"\"\n",
        "    augmented_data = []  # list to store segments\n",
        "    augmented_targets = []  # list to store segment labels\n",
        "\n",
        "    if data.ndim == 3:  # if data has extra dimension\n",
        "        data = data[0]  # remove extra dimension\n",
        "\n",
        "    n_segments = data.shape[1] // segment_size  # calculate number of segments\n",
        "\n",
        "    for i in range(n_segments):  # loop through segments\n",
        "        seg = data[:, i * segment_size:(i + 1) * segment_size]  # extract segment\n",
        "        augmented_data.append(seg.astype(np.float32, copy=False))  # store segment\n",
        "        augmented_targets.append(int(target))  # store label\n",
        "\n",
        "    return augmented_data, augmented_targets  # return segments and labels\n",
        "\n",
        "print(\"[STEP 6] Augmenting all trials...\")  # log augmentation start\n",
        "\n",
        "augmented = []  # list to store all segments\n",
        "aug_targets = []  # list to store all segment labels\n",
        "aug_subject_ids = []  # list to store subject ID for each segment\n",
        "\n",
        "for trial, y, subj in zip(data_clean, targets, subject_ids):  # loop through trials\n",
        "    segs, ys = augment_data(trial, int(y), segment_size=100)  # split into 100-sample segments\n",
        "    augmented.extend(segs)  # add segments to list\n",
        "    aug_targets.extend(ys)  # add labels to list\n",
        "    aug_subject_ids.extend([subj] * len(segs))  # repeat subject ID for all segments\n",
        "\n",
        "augmented = np.array(augmented, dtype=np.float32)  # convert to numpy array (N, C, Tseg)\n",
        "aug_targets = np.array(aug_targets, dtype=np.int32)  # convert labels to numpy array\n",
        "aug_subject_ids = np.array(aug_subject_ids)  # convert subject IDs to numpy array\n",
        "\n",
        "print(\"[STEP 6] Augmentation done.\")  # log completion\n",
        "print(\"  augmented shape:\", augmented.shape)  # log augmented data shape\n",
        "print(\"  aug_targets shape:\", aug_targets.shape)  # log targets shape\n",
        "print(\"  Class counts (augmented):\", np.unique(aug_targets, return_counts=True))  # log class distribution\n",
        "\n",
        "\n",
        "# Cell 7 — Fair selection (limit segments per subject-class)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"[STEP 7] Grouping segments by subject and class...\")  # log grouping start\n",
        "\n",
        "subject_data = defaultdict(lambda: {0: [], 1: []})  # create nested dict: subject -> {class: [segments]}\n",
        "\n",
        "for x, y, subj in zip(augmented, aug_targets, aug_subject_ids):  # loop through all segments\n",
        "    subject_data[subj][int(y)].append(x)  # group segment by subject and class\n",
        "\n",
        "max_per_class_per_subject = 200  # maximum segments to select per subject-class combination\n",
        "\n",
        "selected_data = []  # list to store selected segments\n",
        "selected_targets = []  # list to store selected labels\n",
        "\n",
        "print(\"[STEP 7] Selecting up to 200 segments per (subject, class)...\")  # log selection start\n",
        "\n",
        "for subj in subjects_used:  # loop through all subjects in dataset\n",
        "    for class_label in [0, 1]:  # loop through both classes\n",
        "        samples = subject_data.get(subj, {0: [], 1: []})[class_label]  # get segments for this subject-class\n",
        "        picked = samples[:max_per_class_per_subject]  # select up to 200 segments\n",
        "        selected_data.extend(picked)  # add selected segments\n",
        "        selected_targets.extend([class_label] * len(picked))  # add corresponding labels\n",
        "\n",
        "selected_data = np.array(selected_data, dtype=np.float32)  # convert to numpy array\n",
        "selected_targets = np.array(selected_targets, dtype=np.int32)  # convert labels to numpy array\n",
        "\n",
        "print(\"[STEP 7] Selection done.\")  # log completion\n",
        "print(\"  selected_data shape:\", selected_data.shape)  # log selected data shape\n",
        "print(\"  selected_targets shape:\", selected_targets.shape)  # log targets shape\n",
        "print(\"  Class counts (selected):\", np.unique(selected_targets, return_counts=True))  # log class distribution\n",
        "\n",
        "\n",
        "# Cell 8 — Reshape for CNN input\n",
        "# ============================================================================\n",
        "\n",
        "print(\"[STEP 8] Reshaping for CNN...\")  # log reshaping start\n",
        "\n",
        "# Reshape from (N, C, T) to (N, C, T, 1) for CNN input\n",
        "X_all = selected_data[..., np.newaxis].astype(np.float32, copy=False)  # add channel dimension\n",
        "y_all = selected_targets.astype(np.int32, copy=False)  # keep labels as int32\n",
        "\n",
        "print(\"[STEP 8] Done.\")  # log completion\n",
        "print(\"  X_all shape:\", X_all.shape)  # log final data shape\n",
        "print(\"  y_all shape:\", y_all.shape)  # log final labels shape\n",
        "print(\"  Class counts:\", np.unique(y_all, return_counts=True))  # log class distribution\n",
        "\n",
        "\n",
        "# Cell 9 — EEGNet model definition\n",
        "# ============================================================================\n",
        "\n",
        "def get_lr(opt):\n",
        "    \"\"\"Safely read optimizer learning rate.\"\"\"\n",
        "    try:\n",
        "        return float(tf.keras.backend.get_value(opt.learning_rate))  # standard method\n",
        "    except Exception:\n",
        "        try:\n",
        "            return float(opt.learning_rate.numpy())  # alternative method\n",
        "        except Exception:\n",
        "            try:\n",
        "                return float(opt.lr.numpy())  # legacy method\n",
        "            except Exception:\n",
        "                return float(getattr(opt, \"lr\", 0.0))  # fallback\n",
        "\n",
        "def create_eegnet(input_shape, dropout_rate=0.5, num_classes=1):\n",
        "    \"\"\"Create EEGNet model for binary classification.\"\"\"\n",
        "    n_electrodes = input_shape[0]  # number of EEG channels\n",
        "    inputs = Input(shape=input_shape)  # input layer\n",
        "\n",
        "    # Block 1: Temporal convolution\n",
        "    x = Conv2D(16, (1, 64), padding='same', use_bias=False)(inputs)  # temporal conv\n",
        "    x = BatchNormalization()(x)  # batch normalization\n",
        "\n",
        "    # Block 2: Depthwise spatial convolution\n",
        "    x = DepthwiseConv2D((n_electrodes, 1), depth_multiplier=2, padding='valid', use_bias=False)(x)  # spatial conv\n",
        "    x = BatchNormalization()(x)  # batch normalization\n",
        "    x = Activation('elu')(x)  # ELU activation\n",
        "    x = AveragePooling2D((1, 4))(x)  # pooling\n",
        "    x = Dropout(dropout_rate)(x)  # dropout for regularization\n",
        "\n",
        "    # Block 3: Separable convolution\n",
        "    x = SeparableConv2D(16, (1, 16), padding='same', use_bias=False)(x)  # separable conv\n",
        "    x = BatchNormalization()(x)  # batch normalization\n",
        "    x = Activation('elu')(x)  # ELU activation\n",
        "    x = AveragePooling2D((1, 8))(x)  # pooling\n",
        "    x = Dropout(dropout_rate)(x)  # dropout\n",
        "\n",
        "    # Block 4: Fully connected layers\n",
        "    x = Flatten()(x)  # flatten for dense layers\n",
        "    x = Dense(64, activation='relu')(x)  # dense layer\n",
        "    outputs = Dense(num_classes, activation='sigmoid')(x)  # output layer with sigmoid\n",
        "\n",
        "    return Model(inputs=inputs, outputs=outputs, name=\"EEGNet_simple\")  # return compiled model\n",
        "\n",
        "print(\"✅ Model function defined.\")  # log completion\n",
        "\n",
        "\n",
        "# Cell 10 — Final Model Training (All Data - No CV)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"[STEP 10] Starting final model training with ALL data...\")  # log training start\n",
        "\n",
        "# ---------------- Prepare all data ----------------\n",
        "print(\"[STEP 10] Preparing full dataset...\")  # log preparation start\n",
        "\n",
        "# Use all available data (no train/val split)\n",
        "X_train_raw = X_all.astype(np.float32, copy=False)  # all samples\n",
        "y_train_raw = y_all.astype(np.int32, copy=False)  # all labels\n",
        "\n",
        "# Get input dimensions\n",
        "n_electrodes = X_train_raw.shape[1]  # number of electrodes\n",
        "segment_size = X_train_raw.shape[2]  # segment length\n",
        "input_shape = (n_electrodes, segment_size, 1)  # model input shape\n",
        "\n",
        "print(f\"[STEP 10] Input shape: {input_shape}\")  # log input shape\n",
        "print(f\"[STEP 10] Total samples: {X_train_raw.shape[0]}\")  # log total samples\n",
        "print(f\"[STEP 10] Class count (before balancing): {np.unique(y_train_raw, return_counts=True)}\")  # log class distribution\n",
        "\n",
        "# ---------------- Normalization ----------------\n",
        "print(\"[STEP 10] Computing normalization statistics...\")  # log normalization start\n",
        "\n",
        "epsilon = 1e-6  # small constant to avoid division by zero\n",
        "train_mean = np.mean(X_train_raw, axis=(0, 2, 3), keepdims=True).astype(np.float32)  # mean per electrode\n",
        "train_std = np.std(X_train_raw, axis=(0, 2, 3), keepdims=True).astype(np.float32)  # std per electrode\n",
        "train_std = np.maximum(train_std, epsilon).astype(np.float32)  # clamp std to avoid division by zero\n",
        "\n",
        "X_train_norm = ((X_train_raw - train_mean) / train_std).astype(np.float32)  # normalize data\n",
        "\n",
        "print(f\"[STEP 10] ✅ Normalization done.\")  # log completion\n",
        "\n",
        "# ---------------- SMOTE Balancing ----------------\n",
        "print(f\"[STEP 10] Preparing data for SMOTE (flatten to 2D)...\")  # log SMOTE preparation\n",
        "\n",
        "X_train_2d = X_train_norm.reshape(X_train_norm.shape[0], -1)  # flatten to (N, features)\n",
        "\n",
        "# Check class counts before SMOTE\n",
        "unique_before, counts_before = np.unique(y_train_raw, return_counts=True)  # get class counts\n",
        "print(f\"[STEP 10] Class count BEFORE SMOTE: {dict(zip(unique_before.tolist(), counts_before.tolist()))}\")  # log counts\n",
        "\n",
        "# Determine minority class size\n",
        "minority_n = int(np.min(counts_before))  # smallest class size\n",
        "\n",
        "# Apply SMOTE if possible, otherwise use RandomOverSampler\n",
        "if minority_n >= 2:  # SMOTE requires at least 2 samples in minority class\n",
        "    k_neighbors = max(1, min(5, minority_n - 1))  # calculate safe k_neighbors\n",
        "    smote = SMOTE(random_state=42, k_neighbors=k_neighbors)  # create SMOTE object\n",
        "    X_train_bal_2d, y_train_bal_int = smote.fit_resample(X_train_2d, y_train_raw)  # apply SMOTE\n",
        "    print(f\"[STEP 10] ✅ SMOTE applied (k_neighbors={k_neighbors}).\")  # log SMOTE application\n",
        "else:\n",
        "    ros = RandomOverSampler(random_state=42)  # fallback to random oversampling\n",
        "    X_train_bal_2d, y_train_bal_int = ros.fit_resample(X_train_2d, y_train_raw)  # apply oversampling\n",
        "    print(f\"[STEP 10] ⚠️ SMOTE not possible (minority_n={minority_n}); used RandomOverSampler.\")  # log fallback\n",
        "\n",
        "# Check class counts after balancing\n",
        "unique_after, counts_after = np.unique(y_train_bal_int, return_counts=True)  # get balanced class counts\n",
        "print(f\"[STEP 10] Class count AFTER balancing: {dict(zip(unique_after.tolist(), counts_after.tolist()))}\")  # log balanced counts\n",
        "\n",
        "# Reshape back to CNN format\n",
        "print(f\"[STEP 10] Reshaping back to CNN format...\")  # log reshaping\n",
        "X_train_bal = X_train_bal_2d.reshape(-1, n_electrodes, segment_size, 1).astype(np.float32)  # reshape to (N, C, T, 1)\n",
        "y_train_bal = y_train_bal_int.astype(np.float32)  # convert labels to float32 for Keras\n",
        "\n",
        "print(f\"[STEP 10] ✅ Final training data shape: {X_train_bal.shape}\")  # log final shape\n",
        "print(f\"[STEP 10] ✅ Final labels shape: {y_train_bal.shape}\")  # log labels shape\n",
        "\n",
        "# ---------------- Build Model ----------------\n",
        "print(f\"[STEP 10] Building EEGNet model...\")  # log model building\n",
        "\n",
        "model = create_eegnet(input_shape=input_shape, dropout_rate=0.5, num_classes=1)  # create EEGNet model\n",
        "\n",
        "# Compile model with optimizer, loss, and metrics\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-3),  # Adam optimizer with learning rate 0.001\n",
        "    loss=BinaryCrossentropy(from_logits=False),  # binary cross-entropy loss\n",
        "    metrics=[BinaryAccuracy(name='accuracy', threshold=0.5)]  # binary accuracy metric\n",
        ")\n",
        "\n",
        "print(f\"[STEP 10] ✅ Model compiled. Initial LR={get_lr(model.optimizer)}\")  # log compilation\n",
        "print(model.summary())  # print model architecture summary\n",
        "\n",
        "# ---------------- Setup Callbacks ----------------\n",
        "print(f\"[STEP 10] Setting up training callbacks...\")  # log callback setup\n",
        "\n",
        "# Learning rate reduction on plateau\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor=\"loss\",  # monitor training loss\n",
        "    factor=0.5,  # reduce LR by half\n",
        "    patience=5,  # wait 5 epochs before reducing\n",
        "    min_lr=1e-6,  # minimum learning rate\n",
        "    verbose=1  # print messages\n",
        ")\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"loss\",  # monitor training loss\n",
        "    patience=15,  # wait 15 epochs before stopping\n",
        "    restore_best_weights=True,  # restore best weights\n",
        "    verbose=1  # print messages\n",
        ")\n",
        "\n",
        "# Model checkpoint to save best model\n",
        "ckpt = ModelCheckpoint(\n",
        "    filepath=\"EEGNet-SD-Final.keras\",  # save path\n",
        "    monitor=\"loss\",  # monitor training loss\n",
        "    save_best_only=True,  # only save best model\n",
        "    save_weights_only=False,  # save entire model\n",
        "    verbose=1  # print messages\n",
        ")\n",
        "\n",
        "# CSV logger to track training history\n",
        "csv_logger = CSVLogger(\"final_model_training_log.csv\", append=False)  # log to CSV file\n",
        "\n",
        "callbacks = [lr_scheduler, early_stop, ckpt, csv_logger]  # combine all callbacks\n",
        "\n",
        "print(f\"[STEP 10] ✅ Callbacks configured.\")  # log callback setup complete\n",
        "\n",
        "# ---------------- Train Model ----------------\n",
        "EPOCHS = 300  # maximum number of epochs\n",
        "BATCH_SIZE = 200  # batch size for training\n",
        "\n",
        "print(f\"[STEP 10] Starting training (max {EPOCHS} epochs, batch size {BATCH_SIZE})...\")  # log training start\n",
        "print(\"=\" * 70)  # separator\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_bal, y_train_bal,  # training data and labels\n",
        "    epochs=EPOCHS,  # max epochs\n",
        "    batch_size=BATCH_SIZE,  # batch size\n",
        "    shuffle=True,  # shuffle data each epoch\n",
        "    verbose=1,  # show progress bar\n",
        "    callbacks=callbacks  # apply callbacks\n",
        ")\n",
        "\n",
        "print(\"=\" * 70)  # separator\n",
        "print(f\"[STEP 10] ✅ Training completed!\")  # log training completion\n",
        "\n",
        "# ---------------- Training Summary ----------------\n",
        "print(\"\\n[FINAL] Training Summary:\")  # log summary header\n",
        "print(f\"  Total epochs trained: {len(history.history['loss'])}\")  # log epochs trained\n",
        "print(f\"  Best training loss: {float(np.min(history.history['loss'])):.6f}\")  # log best loss\n",
        "print(f\"  Best training accuracy: {float(np.max(history.history['accuracy'])):.6f}\")  # log best accuracy\n",
        "print(f\"  Final learning rate: {get_lr(model.optimizer):.6e}\")  # log final LR\n",
        "\n",
        "# ---------------- Save Final Model ----------------\n",
        "print(\"\\n[STEP 10] Saving final model...\")  # log save start\n",
        "\n",
        "# Model is already saved via ModelCheckpoint callback\n",
        "# But we can explicitly save again to be sure\n",
        "model.save(\"EEGNet-SD-Final.keras\")  # save model\n",
        "print(f\"[STEP 10] ✅ Model saved as 'EEGNet-SD-Final.keras'\")  # log save completion\n",
        "\n",
        "# ---------------- Plot Training History ----------------\n",
        "print(\"\\n[STEP 10] Plotting training history...\")  # log plotting start\n",
        "\n",
        "plt.figure(figsize=(12, 5))  # create figure\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 1)  # first subplot\n",
        "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)  # plot loss curve\n",
        "plt.xlabel('Epoch')  # x-axis label\n",
        "plt.ylabel('Loss')  # y-axis label\n",
        "plt.title('Training Loss Over Time')  # title\n",
        "plt.legend()  # show legend\n",
        "plt.grid(True, alpha=0.3)  # add grid\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2)  # second subplot\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, color='green')  # plot accuracy curve\n",
        "plt.xlabel('Epoch')  # x-axis label\n",
        "plt.ylabel('Accuracy')  # y-axis label\n",
        "plt.title('Training Accuracy Over Time')  # title\n",
        "plt.legend()  # show legend\n",
        "plt.grid(True, alpha=0.3)  # add grid\n",
        "\n",
        "plt.tight_layout()  # adjust layout\n",
        "plt.savefig('final_model_training_history.png', dpi=300, bbox_inches='tight')  # save figure\n",
        "plt.show()  # display figure\n",
        "\n",
        "print(f\"[STEP 10] ✅ Training history plot saved as 'final_model_training_history.png'\")  # log plot save\n",
        "\n",
        "# ---------------- Evaluate on Training Data ----------------\n",
        "print(\"\\n[STEP 10] Evaluating model on training data...\")  # log evaluation start\n",
        "\n",
        "train_loss, train_acc = model.evaluate(X_train_bal, y_train_bal, batch_size=BATCH_SIZE, verbose=1)  # evaluate model\n",
        "print(f\"[STEP 10] Final training loss: {train_loss:.4f}\")  # log final loss\n",
        "print(f\"[STEP 10] Final training accuracy: {train_acc:.4f}\")  # log final accuracy\n",
        "\n",
        "# Predict on training data\n",
        "print(\"\\n[STEP 10] Generating predictions on training data...\")  # log prediction start\n",
        "y_prob = model.predict(X_train_bal, batch_size=BATCH_SIZE, verbose=1).reshape(-1)  # predict probabilities\n",
        "y_pred = (y_prob > 0.5).astype(int)  # convert to binary predictions\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_train_bal.astype(int), y_pred)  # compute confusion matrix\n",
        "print(f\"\\n[STEP 10] Training Confusion Matrix:\")  # log header\n",
        "print(cm)  # print matrix\n",
        "\n",
        "# Extract metrics from confusion matrix\n",
        "if cm.shape == (2, 2):  # if valid 2x2 matrix\n",
        "    TN, FP = int(cm[0, 0]), int(cm[0, 1])  # true negatives, false positives\n",
        "    FN, TP = int(cm[1, 0]), int(cm[1, 1])  # false negatives, true positives\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = (TP + TN) / max((TP + TN + FP + FN), 1)  # accuracy\n",
        "    precision = TP / max((TP + FP), 1)  # precision\n",
        "    recall = TP / max((TP + FN), 1)  # recall (sensitivity)\n",
        "    specificity = TN / max((TN + FP), 1)  # specificity\n",
        "    f1 = (2 * precision * recall) / max((precision + recall), 1e-12)  # F1 score\n",
        "    \n",
        "    # Calculate AUC\n",
        "    try:\n",
        "        auc_val = float(roc_auc_score(y_train_bal.astype(int), y_prob))  # compute AUC\n",
        "    except Exception:\n",
        "        auc_val = float(\"nan\")  # set to NaN if error\n",
        "        print(f\"[STEP 10] ⚠️ AUC could not be computed.\")  # log warning\n",
        "    \n",
        "    # Print all metrics\n",
        "    print(f\"\\n[FINAL] Training Metrics:\")  # log header\n",
        "    print(f\"  Accuracy:    {accuracy:.4f}\")  # log accuracy\n",
        "    print(f\"  Precision:   {precision:.4f}\")  # log precision\n",
        "    print(f\"  Recall:      {recall:.4f}\")  # log recall\n",
        "    print(f\"  Specificity: {specificity:.4f}\")  # log specificity\n",
        "    print(f\"  F1 Score:    {f1:.4f}\")  # log F1\n",
        "    print(f\"  AUC:         {auc_val:.4f}\")  # log AUC\n",
        "    print(f\"  TN={TN}, FP={FP}, FN={FN}, TP={TP}\")  # log confusion matrix values\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 5))  # create figure\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=True)  # plot heatmap\n",
        "plt.title(\"Final Model - Training Confusion Matrix\")  # title\n",
        "plt.xlabel(\"Predicted\")  # x-axis label\n",
        "plt.ylabel(\"True\")  # y-axis label\n",
        "plt.tight_layout()  # adjust layout\n",
        "plt.savefig('final_model_confusion_matrix.png', dpi=300, bbox_inches='tight')  # save figure\n",
        "plt.show()  # display figure\n",
        "\n",
        "print(f\"[STEP 10] ✅ Confusion matrix plot saved as 'final_model_confusion_matrix.png'\")  # log save\n",
        "\n",
        "# Print classification report\n",
        "print(f\"\\n[STEP 10] Detailed Classification Report:\")  # log header\n",
        "print(classification_report(y_train_bal.astype(int), y_pred, target_names=['Class 0', 'Class 1']))  # print report\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)  # separator\n",
        "print(\"[FINAL] ✅ All training completed successfully!\")  # log completion\n",
        "print(f\"[FINAL] Model saved as: EEGNet-SD-Final.keras\")  # log model path\n",
        "print(f\"[FINAL] Training log saved as: final_model_training_log.csv\")  # log CSV path\n",
        "print(\"=\" * 70)  # separator"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Note: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\n✅ Packages installed (if no errors above).\n✅ Imports done + seeds set.\n✅ Preprocessing classes loaded.\n[STEP 4] Connecting to Azure ML workspace...\n[STEP 4] Getting datastore 'workspaceblobstore'...\n[STEP 4] Downloaded 0 file(s).\n[STEP 4] base_path = ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset\n[STEP 4] Listing base_path contents:\n['CHANGES', 'code', 'dataset_description.json', 'participants.json', 'participants.tsv', 'README', 'session_1_eeg_data.csv', 'session_2_eeg_data.csv', 'sub-01', 'sub-01_eeg_data.pkl'] ...\n\n[STEP 4] Dataset rule summary:\n  ses-1 subjects count: 59 (sub-01..sub-59)\n  ses-2 subjects count: 12 (sub-60..sub-71)\n  total subjects used : 71\n\n[STEP 4] Loading raw EEG data according to rule...\n  Loaded 1 trial(s) from sub-01/ses-1\n  Loaded 1 trial(s) from sub-02/ses-1\n  Loaded 1 trial(s) from sub-03/ses-1\n  Loaded 1 trial(s) from sub-04/ses-1\n  Loaded 1 trial(s) from sub-05/ses-1\n  Loaded 1 trial(s) from sub-06/ses-1\n  Loaded 1 trial(s) from sub-07/ses-1\n  Loaded 1 trial(s) from sub-08/ses-1\n  Loaded 1 trial(s) from sub-09/ses-1\n  Loaded 1 trial(s) from sub-10/ses-1\n  Loaded 1 trial(s) from sub-11/ses-1\n  Loaded 1 trial(s) from sub-12/ses-1\n  Loaded 1 trial(s) from sub-13/ses-1\n  Loaded 1 trial(s) from sub-14/ses-1\n  Loaded 1 trial(s) from sub-15/ses-1\n  Loaded 1 trial(s) from sub-16/ses-1\n  Loaded 1 trial(s) from sub-17/ses-1\n  Loaded 1 trial(s) from sub-18/ses-1\n  Loaded 1 trial(s) from sub-19/ses-1\n  Loaded 1 trial(s) from sub-20/ses-1\n  Loaded 1 trial(s) from sub-21/ses-1\n  Loaded 1 trial(s) from sub-22/ses-1\n  Loaded 1 trial(s) from sub-23/ses-1\n  Loaded 1 trial(s) from sub-24/ses-1\n  Loaded 1 trial(s) from sub-25/ses-1\n  Loaded 1 trial(s) from sub-26/ses-1\n  Loaded 1 trial(s) from sub-27/ses-1\n  Loaded 1 trial(s) from sub-28/ses-1\n  Loaded 1 trial(s) from sub-29/ses-1\n  Loaded 1 trial(s) from sub-30/ses-1\n  Loaded 1 trial(s) from sub-31/ses-1\n  Loaded 1 trial(s) from sub-32/ses-1\n  Loaded 1 trial(s) from sub-33/ses-1\n  Loaded 1 trial(s) from sub-34/ses-1\n  Loaded 1 trial(s) from sub-35/ses-1\n  Loaded 1 trial(s) from sub-36/ses-1\n  Loaded 1 trial(s) from sub-37/ses-1\n  Loaded 1 trial(s) from sub-38/ses-1\n  Loaded 1 trial(s) from sub-39/ses-1\n  Loaded 1 trial(s) from sub-40/ses-1\n  Loaded 1 trial(s) from sub-42/ses-1\n  Loaded 1 trial(s) from sub-44/ses-1\n  Loaded 1 trial(s) from sub-45/ses-1\n  Loaded 1 trial(s) from sub-46/ses-1\n  Loaded 1 trial(s) from sub-47/ses-1\n  Loaded 1 trial(s) from sub-48/ses-1\n  Loaded 1 trial(s) from sub-49/ses-1\n  Loaded 1 trial(s) from sub-50/ses-1\n  Loaded 1 trial(s) from sub-51/ses-1\n  Loaded 1 trial(s) from sub-52/ses-1\n  Loaded 1 trial(s) from sub-53/ses-1\n  Loaded 1 trial(s) from sub-54/ses-1\n  Loaded 1 trial(s) from sub-55/ses-1\n  Loaded 1 trial(s) from sub-56/ses-1\n  Loaded 1 trial(s) from sub-57/ses-1\n  Loaded 1 trial(s) from sub-58/ses-1\n  Loaded 1 trial(s) from sub-59/ses-1\n  Loaded 1 trial(s) from sub-60/ses-2\n  Loaded 1 trial(s) from sub-61/ses-2\n  Loaded 1 trial(s) from sub-62/ses-2\n  Loaded 1 trial(s) from sub-63/ses-2\n  Loaded 1 trial(s) from sub-64/ses-2\n  Loaded 1 trial(s) from sub-65/ses-2\n  Loaded 1 trial(s) from sub-66/ses-2\n  Loaded 1 trial(s) from sub-67/ses-2\n  Loaded 1 trial(s) from sub-68/ses-2\n  Loaded 1 trial(s) from sub-69/ses-2\n  Loaded 1 trial(s) from sub-70/ses-2\n  Loaded 1 trial(s) from sub-71/ses-2\n\n[STEP 4] Finished loading raw trials.\n  Total trials loaded: 71\n  Targets shape: (71,)\n  Unique labels + counts: (array([0, 1], dtype=int32), array([59, 12]))\n[STEP 4] Using fs=500.0 Hz\n[STEP 5] Fitting EEGPreprocessor on a subset of loaded trials...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\n[STEP 5] Preprocessor fitted. Output fs: 500.0 Hz\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\n[STEP 5] Preprocessed 20/71 trials\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\n[STEP 5] Preprocessed 40/71 trials\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\n[STEP 5] Preprocessed 60/71 trials\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\n[STEP 5] Done preprocessing all trials.\n  data_clean shape: (71, 61, 108000)\n  targets shape   : (71,)\n[STEP 6] Augmenting all trials...\n[STEP 6] Augmentation done.\n  augmented shape: (76680, 61, 100)\n  aug_targets shape: (76680,)\n  Class counts (augmented): (array([0, 1], dtype=int32), array([63720, 12960]))\n[STEP 7] Grouping segments by subject and class...\n[STEP 7] Selecting up to 200 segments per (subject, class)...\n[STEP 7] Selection done.\n  selected_data shape: (14200, 61, 100)\n  selected_targets shape: (14200,)\n  Class counts (selected): (array([0, 1], dtype=int32), array([11800,  2400]))\n[STEP 8] Reshaping for CNN...\n[STEP 8] Done.\n  X_all shape: (14200, 61, 100, 1)\n  y_all shape: (14200,)\n  Class counts: (array([0, 1], dtype=int32), array([11800,  2400]))\n✅ Model function defined.\n[STEP 10] Starting final model training with ALL data...\n[STEP 10] Preparing full dataset...\n[STEP 10] Input shape: (61, 100, 1)\n[STEP 10] Total samples: 14200\n[STEP 10] Class count (before balancing): (array([0, 1], dtype=int32), array([11800,  2400]))\n[STEP 10] Computing normalization statistics...\n[STEP 10] ✅ Normalization done.\n[STEP 10] Preparing data for SMOTE (flatten to 2D)...\n[STEP 10] Class count BEFORE SMOTE: {0: 11800, 1: 2400}\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2026-01-09 10:31:53.680437: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2026-01-09 10:31:53.981228: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767954714.091017    3615 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767954714.117276    3615 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767954714.344157    3615 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767954714.344192    3615 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767954714.344194    3615 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767954714.344196    3615 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2026-01-09 10:31:54.394314: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/session_1_eeg_data.csv\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/session_2_eeg_data.csv\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-01/ses-1/sub-01_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-01/ses-1/sub-01_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-01/ses-2/sub-01_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-01/ses-2/sub-01_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-01_eeg_data.pkl\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-02/ses-1/sub-02_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-02/ses-1/sub-02_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-02/ses-2/sub-02_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-02/ses-2/sub-02_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-03/ses-1/sub-03_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-03/ses-1/sub-03_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-03/ses-2/sub-03_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-03/ses-2/sub-03_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-04/ses-1/sub-04_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-04/ses-1/sub-04_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-04/ses-2/sub-04_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-04/ses-2/sub-04_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-05/ses-1/sub-05_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-05/ses-1/sub-05_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-05/ses-2/sub-05_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-05/ses-2/sub-05_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-06/ses-1/sub-06_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-06/ses-1/sub-06_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-06/ses-2/sub-06_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-06/ses-2/sub-06_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-07/ses-1/sub-07_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-07/ses-1/sub-07_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-07/ses-2/sub-07_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-07/ses-2/sub-07_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-08/ses-1/sub-08_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-08/ses-1/sub-08_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-08/ses-2/sub-08_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-08/ses-2/sub-08_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-09/ses-1/sub-09_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-09/ses-1/sub-09_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-09/ses-2/sub-09_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-09/ses-2/sub-09_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-10/ses-1/sub-10_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-10/ses-1/sub-10_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-10/ses-2/sub-10_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-10/ses-2/sub-10_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-11/ses-1/sub-11_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-11/ses-1/sub-11_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-11/ses-2/sub-11_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-11/ses-2/sub-11_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-12/ses-1/sub-12_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-12/ses-1/sub-12_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-12/ses-2/sub-12_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-12/ses-2/sub-12_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-13/ses-1/sub-13_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-13/ses-1/sub-13_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-13/ses-2/sub-13_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-13/ses-2/sub-13_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-14/ses-1/sub-14_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-14/ses-1/sub-14_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-14/ses-2/sub-14_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-14/ses-2/sub-14_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-15/ses-1/sub-15_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-15/ses-1/sub-15_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-15/ses-2/sub-15_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-15/ses-2/sub-15_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-16/ses-1/sub-16_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-16/ses-1/sub-16_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-16/ses-2/sub-16_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-16/ses-2/sub-16_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-17/ses-1/sub-17_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-17/ses-1/sub-17_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-17/ses-2/sub-17_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-17/ses-2/sub-17_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-18/ses-1/sub-18_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-18/ses-1/sub-18_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-18/ses-2/sub-18_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-18/ses-2/sub-18_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-19/ses-1/sub-19_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-19/ses-1/sub-19_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-19/ses-2/sub-19_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-19/ses-2/sub-19_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-20/ses-1/sub-20_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-20/ses-1/sub-20_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-20/ses-2/sub-20_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-20/ses-2/sub-20_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-21/ses-1/sub-21_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-21/ses-1/sub-21_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-21/ses-2/sub-21_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-21/ses-2/sub-21_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-22/ses-1/sub-22_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-22/ses-1/sub-22_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-22/ses-2/sub-22_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-22/ses-2/sub-22_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-23/ses-1/sub-23_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-23/ses-1/sub-23_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-23/ses-2/sub-23_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-23/ses-2/sub-23_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-24/ses-1/sub-24_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-24/ses-1/sub-24_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-24/ses-2/sub-24_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-24/ses-2/sub-24_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-25/ses-1/sub-25_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-25/ses-1/sub-25_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-25/ses-2/sub-25_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-25/ses-2/sub-25_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-26/ses-1/sub-26_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-26/ses-1/sub-26_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-26/ses-2/sub-26_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-26/ses-2/sub-26_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-27/ses-1/sub-27_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-27/ses-1/sub-27_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-27/ses-2/sub-27_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-27/ses-2/sub-27_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-28/ses-1/sub-28_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-28/ses-1/sub-28_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-28/ses-2/sub-28_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-28/ses-2/sub-28_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-29/ses-1/sub-29_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-29/ses-1/sub-29_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-29/ses-2/sub-29_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-29/ses-2/sub-29_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-30/ses-1/sub-30_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-30/ses-1/sub-30_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-30/ses-2/sub-30_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-30/ses-2/sub-30_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-31/ses-1/sub-31_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-31/ses-1/sub-31_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-31/ses-2/sub-31_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-31/ses-2/sub-31_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-32/ses-1/sub-32_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-32/ses-1/sub-32_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-32/ses-2/sub-32_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-32/ses-2/sub-32_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-33/ses-1/sub-33_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-33/ses-1/sub-33_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-33/ses-2/sub-33_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-33/ses-2/sub-33_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-34/ses-1/sub-34_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-34/ses-1/sub-34_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-34/ses-2/sub-34_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-34/ses-2/sub-34_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-35/ses-1/sub-35_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-35/ses-1/sub-35_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-35/ses-2/sub-35_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-35/ses-2/sub-35_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-36/ses-1/sub-36_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-36/ses-1/sub-36_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-36/ses-2/sub-36_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-36/ses-2/sub-36_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-37/ses-1/sub-37_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-37/ses-1/sub-37_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-37/ses-2/sub-37_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-37/ses-2/sub-37_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-38/ses-1/sub-38_ses-1_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-38/ses-1/sub-38_ses-1_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-38/ses-2/sub-38_ses-2_task-eyesclosed_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-38/ses-2/sub-38_ses-2_task-eyesclosed_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-39/ses-1/sub-39_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-39/ses-1/sub-39_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-39/ses-2/sub-39_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-39/ses-2/sub-39_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-40/ses-1/sub-40_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-40/ses-1/sub-40_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-40/ses-2/sub-40_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-40/ses-2/sub-40_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-41/ses-1/sub-41_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-41/ses-1/sub-41_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-41/ses-2/sub-41_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-41/ses-2/sub-41_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-42/ses-1/sub-42_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-42/ses-1/sub-42_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-42/ses-2/sub-42_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-42/ses-2/sub-42_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-43/ses-1/sub-43_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-43/ses-1/sub-43_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-43/ses-2/sub-43_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-43/ses-2/sub-43_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-44/ses-1/sub-44_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-44/ses-1/sub-44_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-44/ses-2/sub-44_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-44/ses-2/sub-44_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-45/ses-1/sub-45_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-45/ses-1/sub-45_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-45/ses-2/sub-45_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-45/ses-2/sub-45_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-46/ses-1/sub-46_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-46/ses-1/sub-46_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-46/ses-2/sub-46_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-46/ses-2/sub-46_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-47/ses-1/sub-47_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-47/ses-1/sub-47_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-47/ses-2/sub-47_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-47/ses-2/sub-47_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-48/ses-1/sub-48_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-48/ses-1/sub-48_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-48/ses-2/sub-48_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-48/ses-2/sub-48_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-49/ses-1/sub-49_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-49/ses-1/sub-49_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-49/ses-2/sub-49_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-49/ses-2/sub-49_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-50/ses-1/sub-50_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-50/ses-1/sub-50_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-50/ses-2/sub-50_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-50/ses-2/sub-50_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-51/ses-1/sub-51_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-51/ses-1/sub-51_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-51/ses-2/sub-51_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-51/ses-2/sub-51_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-52/ses-1/sub-52_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-52/ses-1/sub-52_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-52/ses-2/sub-52_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-52/ses-2/sub-52_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-53/ses-1/sub-53_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-53/ses-1/sub-53_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-53/ses-2/sub-53_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-53/ses-2/sub-53_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-54/ses-1/sub-54_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-54/ses-1/sub-54_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-54/ses-2/sub-54_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-54/ses-2/sub-54_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-55/ses-1/sub-55_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-55/ses-1/sub-55_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-55/ses-2/sub-55_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-55/ses-2/sub-55_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-56/ses-1/sub-56_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-56/ses-1/sub-56_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-56/ses-2/sub-56_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-56/ses-2/sub-56_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-57/ses-1/sub-57_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-57/ses-1/sub-57_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-57/ses-2/sub-57_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-57/ses-2/sub-57_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-58/ses-1/sub-58_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-58/ses-1/sub-58_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-58/ses-2/sub-58_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-58/ses-2/sub-58_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-59/ses-1/sub-59_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-59/ses-1/sub-59_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-59/ses-2/sub-59_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-59/ses-2/sub-59_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-60/ses-1/sub-60_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-60/ses-1/sub-60_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-60/ses-2/sub-60_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-60/ses-2/sub-60_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-61/ses-1/sub-61_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-61/ses-1/sub-61_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-61/ses-2/sub-61_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-61/ses-2/sub-61_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-62/ses-1/sub-62_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-62/ses-1/sub-62_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-62/ses-2/sub-62_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-62/ses-2/sub-62_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-63/ses-1/sub-63_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-63/ses-1/sub-63_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-63/ses-2/sub-63_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-63/ses-2/sub-63_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-64/ses-1/sub-64_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-64/ses-1/sub-64_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-64/ses-2/sub-64_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-64/ses-2/sub-64_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-65/ses-1/sub-65_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-65/ses-1/sub-65_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-65/ses-2/sub-65_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-65/ses-2/sub-65_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-66/ses-1/sub-66_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-66/ses-1/sub-66_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-66/ses-2/sub-66_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-66/ses-2/sub-66_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-67/ses-1/sub-67_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-67/ses-1/sub-67_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-67/ses-2/sub-67_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-67/ses-2/sub-67_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-68/ses-1/sub-68_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-68/ses-1/sub-68_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-68/ses-2/sub-68_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-68/ses-2/sub-68_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-69/ses-1/sub-69_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-69/ses-1/sub-69_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-69/ses-2/sub-69_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-69/ses-2/sub-69_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-70/ses-1/sub-70_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-70/ses-1/sub-70_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-70/ses-2/sub-70_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-70/ses-2/sub-70_ses-2_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-71/ses-1/sub-71_ses-1_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-71/ses-1/sub-71_ses-1_task-eyesopen_eeg.set\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-71/ses-2/sub-71_ses-2_task-eyesopen_eeg.fdt\nPath already exists. Skipping download for ./azureml_eeg_data/UI/2025-12-11_033542_UTC/New Dataset/sub-71/ses-2/sub-71_ses-2_task-eyesopen_eeg.set\n/tmp/ipykernel_3615/2647004444.py:81: RuntimeWarning: Data file name in EEG.data (sub-41_ses-1_task-eyesopen_eeg.fdt) is incorrect, the file name must have changed on disk, using the correct file name (sub-42_ses-1_task-eyesopen_eeg.fdt).\n  raw = mne.io.read_raw_eeglab(file_path, preload=True, verbose=False)  # load EEG file\n/tmp/ipykernel_3615/2647004444.py:81: RuntimeWarning: Data file name in EEG.data (sub-43_ses-1_task-eyesopen_eeg.fdt) is incorrect, the file name must have changed on disk, using the correct file name (sub-44_ses-1_task-eyesopen_eeg.fdt).\n  raw = mne.io.read_raw_eeglab(file_path, preload=True, verbose=False)  # load EEG file\n2026-01-09 10:34:54.401005: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1mModel: \"EEGNet_simple\"\u001b[0m\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EEGNet_simple\"</span>\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │         \u001b[38;5;34m1,024\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │            \u001b[38;5;34m64\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ depthwise_conv2d                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m1,952\u001b[0m │\n│ (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ average_pooling2d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m32\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m32\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ separable_conv2d                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m16\u001b[0m)      │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m16\u001b[0m)      │            \u001b[38;5;34m64\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m16\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ average_pooling2d_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m3,136\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ depthwise_conv2d                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,952</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ average_pooling2d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ separable_conv2d                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ average_pooling2d_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,457\u001b[0m (29.13 KB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,457</span> (29.13 KB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,329\u001b[0m (28.63 KB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,329</span> (28.63 KB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "None\n[STEP 10] Setting up training callbacks...\n[STEP 10] ✅ Callbacks configured.\n[STEP 10] Starting training (max 300 epochs, batch size 200)...\n======================================================================\nEpoch 1/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - accuracy: 0.6075 - loss: 0.6616\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 1: loss improved from inf to 0.61657, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 276ms/step - accuracy: 0.6079 - loss: 0.6612 - learning_rate: 0.0010\nEpoch 2/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.7497 - loss: 0.5178\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 2: loss improved from 0.61657 to 0.48600, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 273ms/step - accuracy: 0.7498 - loss: 0.5175 - learning_rate: 0.0010\nEpoch 3/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.8278 - loss: 0.3832\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 3: loss improved from 0.48600 to 0.36478, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 275ms/step - accuracy: 0.8279 - loss: 0.3830 - learning_rate: 0.0010\nEpoch 4/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.8678 - loss: 0.3139\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 4: loss improved from 0.36478 to 0.30317, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 274ms/step - accuracy: 0.8679 - loss: 0.3138 - learning_rate: 0.0010\nEpoch 5/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - accuracy: 0.8840 - loss: 0.2760\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 5: loss improved from 0.30317 to 0.27066, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 279ms/step - accuracy: 0.8840 - loss: 0.2760 - learning_rate: 0.0010\nEpoch 6/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.9003 - loss: 0.2440\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 6: loss improved from 0.27066 to 0.24494, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 275ms/step - accuracy: 0.9003 - loss: 0.2440 - learning_rate: 0.0010\nEpoch 7/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - accuracy: 0.9062 - loss: 0.2277\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 7: loss improved from 0.24494 to 0.22584, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 272ms/step - accuracy: 0.9062 - loss: 0.2276 - learning_rate: 0.0010\nEpoch 8/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.9180 - loss: 0.2092\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 8: loss improved from 0.22584 to 0.20588, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 275ms/step - accuracy: 0.9180 - loss: 0.2091 - learning_rate: 0.0010\nEpoch 9/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.9233 - loss: 0.1977\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 9: loss improved from 0.20588 to 0.19515, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 273ms/step - accuracy: 0.9233 - loss: 0.1977 - learning_rate: 0.0010\nEpoch 10/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.9250 - loss: 0.1874\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 10: loss improved from 0.19515 to 0.18221, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 274ms/step - accuracy: 0.9251 - loss: 0.1874 - learning_rate: 0.0010\nEpoch 11/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.9287 - loss: 0.1811\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 11: loss improved from 0.18221 to 0.18036, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 275ms/step - accuracy: 0.9287 - loss: 0.1811 - learning_rate: 0.0010\nEpoch 12/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.9336 - loss: 0.1705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 12: loss improved from 0.18036 to 0.17364, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 273ms/step - accuracy: 0.9336 - loss: 0.1705 - learning_rate: 0.0010\nEpoch 13/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - accuracy: 0.9354 - loss: 0.1700\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 13: loss improved from 0.17364 to 0.16896, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 276ms/step - accuracy: 0.9354 - loss: 0.1700 - learning_rate: 0.0010\nEpoch 14/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.9374 - loss: 0.1639\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 14: loss improved from 0.16896 to 0.16287, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 280ms/step - accuracy: 0.9374 - loss: 0.1639 - learning_rate: 0.0010\nEpoch 15/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.9405 - loss: 0.1574\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 15: loss improved from 0.16287 to 0.15515, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 274ms/step - accuracy: 0.9404 - loss: 0.1574 - learning_rate: 0.0010\nEpoch 16/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.9412 - loss: 0.1546\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 16: loss improved from 0.15515 to 0.15200, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 273ms/step - accuracy: 0.9412 - loss: 0.1546 - learning_rate: 0.0010\nEpoch 17/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.9408 - loss: 0.1487\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 17: loss did not improve from 0.15200\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 273ms/step - accuracy: 0.9408 - loss: 0.1487 - learning_rate: 0.0010\nEpoch 18/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.9457 - loss: 0.1480\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 18: loss improved from 0.15200 to 0.14707, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 273ms/step - accuracy: 0.9457 - loss: 0.1480 - learning_rate: 0.0010\nEpoch 19/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.9435 - loss: 0.1452\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 19: loss improved from 0.14707 to 0.14388, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 273ms/step - accuracy: 0.9435 - loss: 0.1452 - learning_rate: 0.0010\nEpoch 20/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.9483 - loss: 0.1382\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 20: loss improved from 0.14388 to 0.13948, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 275ms/step - accuracy: 0.9483 - loss: 0.1383 - learning_rate: 0.0010\nEpoch 21/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.9471 - loss: 0.1370\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 21: loss did not improve from 0.13948\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 273ms/step - accuracy: 0.9471 - loss: 0.1371 - learning_rate: 0.0010\nEpoch 22/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.9475 - loss: 0.1371\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 22: loss improved from 0.13948 to 0.13488, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 274ms/step - accuracy: 0.9475 - loss: 0.1371 - learning_rate: 0.0010\nEpoch 23/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - accuracy: 0.9457 - loss: 0.1381\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 23: loss did not improve from 0.13488\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 277ms/step - accuracy: 0.9457 - loss: 0.1381 - learning_rate: 0.0010\nEpoch 24/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.9512 - loss: 0.1304\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 24: loss improved from 0.13488 to 0.13014, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 273ms/step - accuracy: 0.9512 - loss: 0.1304 - learning_rate: 0.0010\nEpoch 25/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.9511 - loss: 0.1309\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 25: loss did not improve from 0.13014\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 273ms/step - accuracy: 0.9511 - loss: 0.1309 - learning_rate: 0.0010\nEpoch 26/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.9532 - loss: 0.1241\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 26: loss improved from 0.13014 to 0.12430, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 273ms/step - accuracy: 0.9532 - loss: 0.1241 - learning_rate: 0.0010\nEpoch 27/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.9536 - loss: 0.1218\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 27: loss improved from 0.12430 to 0.12372, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 274ms/step - accuracy: 0.9536 - loss: 0.1218 - learning_rate: 0.0010\nEpoch 28/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.9526 - loss: 0.1257\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 28: loss did not improve from 0.12372\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 273ms/step - accuracy: 0.9526 - loss: 0.1257 - learning_rate: 0.0010\nEpoch 29/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.9573 - loss: 0.1173\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 29: loss improved from 0.12372 to 0.11752, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 275ms/step - accuracy: 0.9573 - loss: 0.1173 - learning_rate: 0.0010\nEpoch 30/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.9535 - loss: 0.1249\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 30: loss did not improve from 0.11752\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 273ms/step - accuracy: 0.9535 - loss: 0.1249 - learning_rate: 0.0010\nEpoch 31/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.9581 - loss: 0.1155\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 31: loss did not improve from 0.11752\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 272ms/step - accuracy: 0.9581 - loss: 0.1156 - learning_rate: 0.0010\nEpoch 32/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.9554 - loss: 0.1224\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 32: loss did not improve from 0.11752\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 279ms/step - accuracy: 0.9554 - loss: 0.1224 - learning_rate: 0.0010\nEpoch 33/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.9570 - loss: 0.1151\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 33: loss improved from 0.11752 to 0.11303, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 276ms/step - accuracy: 0.9570 - loss: 0.1151 - learning_rate: 0.0010\nEpoch 34/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.9597 - loss: 0.1148\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 34: loss did not improve from 0.11303\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 273ms/step - accuracy: 0.9596 - loss: 0.1148 - learning_rate: 0.0010\nEpoch 35/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.9555 - loss: 0.1173\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 35: loss did not improve from 0.11303\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 272ms/step - accuracy: 0.9555 - loss: 0.1173 - learning_rate: 0.0010\nEpoch 36/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.9585 - loss: 0.1139\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 36: loss improved from 0.11303 to 0.11224, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 273ms/step - accuracy: 0.9585 - loss: 0.1139 - learning_rate: 0.0010\nEpoch 37/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.9617 - loss: 0.1062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 37: loss improved from 0.11224 to 0.10634, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 276ms/step - accuracy: 0.9617 - loss: 0.1062 - learning_rate: 0.0010\nEpoch 38/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.9600 - loss: 0.1056\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 38: loss did not improve from 0.10634\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 276ms/step - accuracy: 0.9600 - loss: 0.1056 - learning_rate: 0.0010\nEpoch 39/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.9623 - loss: 0.1042\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 39: loss improved from 0.10634 to 0.10593, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 274ms/step - accuracy: 0.9623 - loss: 0.1042 - learning_rate: 0.0010\nEpoch 40/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.9611 - loss: 0.1068\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 40: loss did not improve from 0.10593\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 272ms/step - accuracy: 0.9611 - loss: 0.1068 - learning_rate: 0.0010\nEpoch 41/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - accuracy: 0.9616 - loss: 0.1047\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 41: loss improved from 0.10593 to 0.10392, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 278ms/step - accuracy: 0.9616 - loss: 0.1047 - learning_rate: 0.0010\nEpoch 42/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.9631 - loss: 0.1037\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 42: loss did not improve from 0.10392\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 273ms/step - accuracy: 0.9631 - loss: 0.1037 - learning_rate: 0.0010\nEpoch 43/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.9617 - loss: 0.1024\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 43: loss improved from 0.10392 to 0.10227, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 284ms/step - accuracy: 0.9617 - loss: 0.1024 - learning_rate: 0.0010\nEpoch 44/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - accuracy: 0.9631 - loss: 0.0996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 44: loss did not improve from 0.10227\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 277ms/step - accuracy: 0.9631 - loss: 0.0997 - learning_rate: 0.0010\nEpoch 45/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.9638 - loss: 0.1017\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 45: loss improved from 0.10227 to 0.10174, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 273ms/step - accuracy: 0.9638 - loss: 0.1017 - learning_rate: 0.0010\nEpoch 46/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.9621 - loss: 0.0987\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 46: loss improved from 0.10174 to 0.09969, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 277ms/step - accuracy: 0.9621 - loss: 0.0987 - learning_rate: 0.0010\nEpoch 47/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - accuracy: 0.9633 - loss: 0.0984\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 47: loss improved from 0.09969 to 0.09936, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 276ms/step - accuracy: 0.9633 - loss: 0.0984 - learning_rate: 0.0010\nEpoch 48/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - accuracy: 0.9653 - loss: 0.0939\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 48: loss improved from 0.09936 to 0.09655, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 276ms/step - accuracy: 0.9653 - loss: 0.0939 - learning_rate: 0.0010\nEpoch 49/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - accuracy: 0.9669 - loss: 0.0909\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 49: loss did not improve from 0.09655\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 275ms/step - accuracy: 0.9669 - loss: 0.0909 - learning_rate: 0.0010\nEpoch 50/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - accuracy: 0.9644 - loss: 0.0948\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 50: loss improved from 0.09655 to 0.09476, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 279ms/step - accuracy: 0.9644 - loss: 0.0948 - learning_rate: 0.0010\nEpoch 51/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.9646 - loss: 0.0930\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 51: loss improved from 0.09476 to 0.09179, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 275ms/step - accuracy: 0.9646 - loss: 0.0929 - learning_rate: 0.0010\nEpoch 52/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.9656 - loss: 0.0909\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 52: loss improved from 0.09179 to 0.09117, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 274ms/step - accuracy: 0.9656 - loss: 0.0909 - learning_rate: 0.0010\nEpoch 53/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.9674 - loss: 0.0929\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 53: loss did not improve from 0.09117\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 277ms/step - accuracy: 0.9674 - loss: 0.0929 - learning_rate: 0.0010\nEpoch 54/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.9687 - loss: 0.0875\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 54: loss improved from 0.09117 to 0.08794, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 274ms/step - accuracy: 0.9687 - loss: 0.0875 - learning_rate: 0.0010\nEpoch 55/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.9645 - loss: 0.0946\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 55: loss did not improve from 0.08794\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 272ms/step - accuracy: 0.9645 - loss: 0.0946 - learning_rate: 0.0010\nEpoch 56/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.9685 - loss: 0.0853\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 57: loss did not improve from 0.08794\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 275ms/step - accuracy: 0.9685 - loss: 0.0854 - learning_rate: 0.0010\nEpoch 58/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - accuracy: 0.9700 - loss: 0.0831\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 58: loss improved from 0.08794 to 0.08529, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 272ms/step - accuracy: 0.9700 - loss: 0.0831 - learning_rate: 0.0010\nEpoch 59/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.9712 - loss: 0.0770\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 59: loss improved from 0.08529 to 0.07963, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 280ms/step - accuracy: 0.9712 - loss: 0.0770 - learning_rate: 0.0010\nEpoch 60/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.9664 - loss: 0.0884\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 60: loss did not improve from 0.07963\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 273ms/step - accuracy: 0.9664 - loss: 0.0884 - learning_rate: 0.0010\nEpoch 61/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.9691 - loss: 0.0850\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 61: loss did not improve from 0.07963\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 274ms/step - accuracy: 0.9691 - loss: 0.0850 - learning_rate: 0.0010\nEpoch 62/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.9710 - loss: 0.0812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 62: loss did not improve from 0.07963\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 274ms/step - accuracy: 0.9710 - loss: 0.0812 - learning_rate: 0.0010\nEpoch 63/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9689 - loss: 0.0858\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 63: loss did not improve from 0.07963\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 263ms/step - accuracy: 0.9689 - loss: 0.0858 - learning_rate: 0.0010\nEpoch 64/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9713 - loss: 0.0812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 64: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 64: loss did not improve from 0.07963\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9713 - loss: 0.0812 - learning_rate: 0.0010\nEpoch 65/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.9716 - loss: 0.0770\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 65: loss improved from 0.07963 to 0.07913, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 264ms/step - accuracy: 0.9716 - loss: 0.0770 - learning_rate: 5.0000e-04\nEpoch 66/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9737 - loss: 0.0743\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 66: loss improved from 0.07913 to 0.07590, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 263ms/step - accuracy: 0.9737 - loss: 0.0743 - learning_rate: 5.0000e-04\nEpoch 67/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9730 - loss: 0.0756\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 67: loss did not improve from 0.07590\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9730 - loss: 0.0756 - learning_rate: 5.0000e-04\nEpoch 68/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.9747 - loss: 0.0695\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 68: loss improved from 0.07590 to 0.07130, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 265ms/step - accuracy: 0.9747 - loss: 0.0695 - learning_rate: 5.0000e-04\nEpoch 69/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.9723 - loss: 0.0729\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 69: loss did not improve from 0.07130\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 264ms/step - accuracy: 0.9724 - loss: 0.0729 - learning_rate: 5.0000e-04\nEpoch 70/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9760 - loss: 0.0751\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 70: loss did not improve from 0.07130\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9760 - loss: 0.0750 - learning_rate: 5.0000e-04\nEpoch 71/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9754 - loss: 0.0665\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 71: loss improved from 0.07130 to 0.07105, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 263ms/step - accuracy: 0.9754 - loss: 0.0666 - learning_rate: 5.0000e-04\nEpoch 72/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.9727 - loss: 0.0717\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 72: loss improved from 0.07105 to 0.06869, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 264ms/step - accuracy: 0.9728 - loss: 0.0716 - learning_rate: 5.0000e-04\nEpoch 73/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9759 - loss: 0.0639\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 73: loss improved from 0.06869 to 0.06687, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9759 - loss: 0.0639 - learning_rate: 5.0000e-04\nEpoch 74/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.9754 - loss: 0.0691\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 74: loss did not improve from 0.06687\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 263ms/step - accuracy: 0.9754 - loss: 0.0691 - learning_rate: 5.0000e-04\nEpoch 75/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9755 - loss: 0.0660\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 75: loss did not improve from 0.06687\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9755 - loss: 0.0661 - learning_rate: 5.0000e-04\nEpoch 76/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.9757 - loss: 0.0655\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 76: loss improved from 0.06687 to 0.06567, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 264ms/step - accuracy: 0.9757 - loss: 0.0655 - learning_rate: 5.0000e-04\nEpoch 77/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9782 - loss: 0.0642\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 77: loss did not improve from 0.06567\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9782 - loss: 0.0643 - learning_rate: 5.0000e-04\nEpoch 78/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.9772 - loss: 0.0639\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 78: loss did not improve from 0.06567\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 265ms/step - accuracy: 0.9772 - loss: 0.0640 - learning_rate: 5.0000e-04\nEpoch 79/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9761 - loss: 0.0661\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 79: loss did not improve from 0.06567\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9761 - loss: 0.0661 - learning_rate: 5.0000e-04\nEpoch 80/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9767 - loss: 0.0658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 80: loss did not improve from 0.06567\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9767 - loss: 0.0658 - learning_rate: 5.0000e-04\nEpoch 81/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9796 - loss: 0.0591\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 81: loss improved from 0.06567 to 0.05995, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9796 - loss: 0.0591 - learning_rate: 5.0000e-04\nEpoch 82/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9732 - loss: 0.0701\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 82: loss did not improve from 0.05995\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9732 - loss: 0.0701 - learning_rate: 5.0000e-04\nEpoch 83/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9792 - loss: 0.0589\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 83: loss did not improve from 0.05995\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9792 - loss: 0.0589 - learning_rate: 5.0000e-04\nEpoch 84/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9770 - loss: 0.0624\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 84: loss did not improve from 0.05995\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9770 - loss: 0.0625 - learning_rate: 5.0000e-04\nEpoch 85/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9772 - loss: 0.0625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 85: loss did not improve from 0.05995\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9772 - loss: 0.0625 - learning_rate: 5.0000e-04\nEpoch 86/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9793 - loss: 0.0584\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 86: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\nEpoch 86: loss did not improve from 0.05995\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9793 - loss: 0.0585 - learning_rate: 5.0000e-04\nEpoch 87/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9803 - loss: 0.0563\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 87: loss improved from 0.05995 to 0.05741, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9803 - loss: 0.0563 - learning_rate: 2.5000e-04\nEpoch 88/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.9782 - loss: 0.0582\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 88: loss did not improve from 0.05741\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 264ms/step - accuracy: 0.9782 - loss: 0.0583 - learning_rate: 2.5000e-04\nEpoch 89/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9799 - loss: 0.0568\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 89: loss did not improve from 0.05741\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 260ms/step - accuracy: 0.9799 - loss: 0.0568 - learning_rate: 2.5000e-04\nEpoch 90/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9791 - loss: 0.0576\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 90: loss did not improve from 0.05741\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 260ms/step - accuracy: 0.9791 - loss: 0.0576 - learning_rate: 2.5000e-04\nEpoch 91/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9819 - loss: 0.0523\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 91: loss improved from 0.05741 to 0.05377, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9819 - loss: 0.0523 - learning_rate: 2.5000e-04\nEpoch 92/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9814 - loss: 0.0525\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 92: loss did not improve from 0.05377\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9814 - loss: 0.0525 - learning_rate: 2.5000e-04\nEpoch 93/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9807 - loss: 0.0576\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 93: loss did not improve from 0.05377\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9807 - loss: 0.0576 - learning_rate: 2.5000e-04\nEpoch 94/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9814 - loss: 0.0519\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 94: loss did not improve from 0.05377\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 260ms/step - accuracy: 0.9814 - loss: 0.0519 - learning_rate: 2.5000e-04\nEpoch 95/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9815 - loss: 0.0531\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 95: loss did not improve from 0.05377\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 261ms/step - accuracy: 0.9815 - loss: 0.0531 - learning_rate: 2.5000e-04\nEpoch 96/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9797 - loss: 0.0580\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 96: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\nEpoch 96: loss did not improve from 0.05377\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 260ms/step - accuracy: 0.9797 - loss: 0.0580 - learning_rate: 2.5000e-04\nEpoch 97/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.9814 - loss: 0.0544\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 97: loss did not improve from 0.05377\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 266ms/step - accuracy: 0.9814 - loss: 0.0544 - learning_rate: 1.2500e-04\nEpoch 98/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9814 - loss: 0.0530\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 98: loss improved from 0.05377 to 0.05348, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9814 - loss: 0.0530 - learning_rate: 1.2500e-04\nEpoch 99/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - accuracy: 0.9813 - loss: 0.0532\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 99: loss improved from 0.05348 to 0.05255, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 270ms/step - accuracy: 0.9813 - loss: 0.0532 - learning_rate: 1.2500e-04\nEpoch 100/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9828 - loss: 0.0500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 100: loss did not improve from 0.05255\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 263ms/step - accuracy: 0.9828 - loss: 0.0500 - learning_rate: 1.2500e-04\nEpoch 101/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9838 - loss: 0.0495\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 101: loss improved from 0.05255 to 0.05026, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9838 - loss: 0.0495 - learning_rate: 1.2500e-04\nEpoch 102/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.9814 - loss: 0.0503\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 102: loss did not improve from 0.05026\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 264ms/step - accuracy: 0.9814 - loss: 0.0504 - learning_rate: 1.2500e-04\nEpoch 103/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9818 - loss: 0.0536\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 103: loss did not improve from 0.05026\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9818 - loss: 0.0536 - learning_rate: 1.2500e-04\nEpoch 104/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9815 - loss: 0.0530\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 104: loss did not improve from 0.05026\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9815 - loss: 0.0530 - learning_rate: 1.2500e-04\nEpoch 105/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9799 - loss: 0.0529\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 105: loss did not improve from 0.05026\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 260ms/step - accuracy: 0.9799 - loss: 0.0529 - learning_rate: 1.2500e-04\nEpoch 106/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.9835 - loss: 0.0523\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 106: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n\nEpoch 106: loss did not improve from 0.05026\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 264ms/step - accuracy: 0.9835 - loss: 0.0523 - learning_rate: 1.2500e-04\nEpoch 107/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9817 - loss: 0.0506\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 107: loss did not improve from 0.05026\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9817 - loss: 0.0506 - learning_rate: 6.2500e-05\nEpoch 108/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.9832 - loss: 0.0482\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 108: loss improved from 0.05026 to 0.05020, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 265ms/step - accuracy: 0.9832 - loss: 0.0482 - learning_rate: 6.2500e-05\nEpoch 109/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.9845 - loss: 0.0478\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 109: loss improved from 0.05020 to 0.04868, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 264ms/step - accuracy: 0.9845 - loss: 0.0479 - learning_rate: 6.2500e-05\nEpoch 110/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9821 - loss: 0.0523\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 110: loss did not improve from 0.04868\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9821 - loss: 0.0523 - learning_rate: 6.2500e-05\nEpoch 111/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9819 - loss: 0.0489\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 111: loss did not improve from 0.04868\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9819 - loss: 0.0489 - learning_rate: 6.2500e-05\nEpoch 112/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9823 - loss: 0.0477\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 112: loss improved from 0.04868 to 0.04778, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 262ms/step - accuracy: 0.9823 - loss: 0.0477 - learning_rate: 6.2500e-05\nEpoch 113/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9836 - loss: 0.0481\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 113: loss did not improve from 0.04778\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9836 - loss: 0.0482 - learning_rate: 6.2500e-05\nEpoch 114/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9818 - loss: 0.0499\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 114: loss did not improve from 0.04778\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9818 - loss: 0.0499 - learning_rate: 6.2500e-05\nEpoch 115/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9822 - loss: 0.0474\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 115: loss did not improve from 0.04778\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 260ms/step - accuracy: 0.9822 - loss: 0.0474 - learning_rate: 6.2500e-05\nEpoch 116/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.9831 - loss: 0.0484\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 116: loss improved from 0.04778 to 0.04767, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 266ms/step - accuracy: 0.9831 - loss: 0.0484 - learning_rate: 6.2500e-05\nEpoch 117/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9842 - loss: 0.0460\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 117: loss did not improve from 0.04767\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9842 - loss: 0.0461 - learning_rate: 6.2500e-05\nEpoch 118/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9823 - loss: 0.0485\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 118: loss did not improve from 0.04767\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 260ms/step - accuracy: 0.9823 - loss: 0.0485 - learning_rate: 6.2500e-05\nEpoch 119/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9808 - loss: 0.0498\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 119: loss did not improve from 0.04767\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9808 - loss: 0.0498 - learning_rate: 6.2500e-05\nEpoch 120/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9837 - loss: 0.0452\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 120: loss improved from 0.04767 to 0.04650, saving model to EEGNet-SD-Final.keras\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9837 - loss: 0.0452 - learning_rate: 6.2500e-05\nEpoch 121/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9839 - loss: 0.0486\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 121: loss did not improve from 0.04650\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9839 - loss: 0.0486 - learning_rate: 6.2500e-05\nEpoch 122/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9829 - loss: 0.0500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 122: loss did not improve from 0.04650\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 260ms/step - accuracy: 0.9829 - loss: 0.0500 - learning_rate: 6.2500e-05\nEpoch 123/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9839 - loss: 0.0469\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 123: loss did not improve from 0.04650\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9839 - loss: 0.0470 - learning_rate: 6.2500e-05\nEpoch 124/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9810 - loss: 0.0536\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 124: loss did not improve from 0.04650\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9810 - loss: 0.0536 - learning_rate: 6.2500e-05\nEpoch 125/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.9839 - loss: 0.0472\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 125: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n\nEpoch 125: loss did not improve from 0.04650\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 264ms/step - accuracy: 0.9839 - loss: 0.0472 - learning_rate: 6.2500e-05\nEpoch 126/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9839 - loss: 0.0466\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 126: loss did not improve from 0.04650\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9839 - loss: 0.0466 - learning_rate: 3.1250e-05\nEpoch 127/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9811 - loss: 0.0497\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 127: loss did not improve from 0.04650\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9811 - loss: 0.0497 - learning_rate: 3.1250e-05\nEpoch 128/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9828 - loss: 0.0483\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 128: loss did not improve from 0.04650\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 260ms/step - accuracy: 0.9828 - loss: 0.0483 - learning_rate: 3.1250e-05\nEpoch 129/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9835 - loss: 0.0458\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 129: loss did not improve from 0.04650\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 262ms/step - accuracy: 0.9835 - loss: 0.0458 - learning_rate: 3.1250e-05\nEpoch 130/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9834 - loss: 0.0463\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 130: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n\nEpoch 130: loss did not improve from 0.04650\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9834 - loss: 0.0463 - learning_rate: 3.1250e-05\nEpoch 131/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9839 - loss: 0.0456\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 131: loss did not improve from 0.04650\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9839 - loss: 0.0456 - learning_rate: 1.5625e-05\nEpoch 132/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9836 - loss: 0.0472\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 132: loss did not improve from 0.04650\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.9836 - loss: 0.0472 - learning_rate: 1.5625e-05\nEpoch 133/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.9819 - loss: 0.0471\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 133: loss did not improve from 0.04650\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 261ms/step - accuracy: 0.9819 - loss: 0.0471 - learning_rate: 1.5625e-05\nEpoch 134/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.9844 - loss: 0.0461\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 134: loss did not improve from 0.04650\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9844 - loss: 0.0462 - learning_rate: 1.5625e-05\nEpoch 135/300\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.9847 - loss: 0.0440\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 135: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n\nEpoch 135: loss did not improve from 0.04650\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 263ms/step - accuracy: 0.9847 - loss: 0.0441 - learning_rate: 1.5625e-05\nEpoch 135: early stopping\nRestoring model weights from the end of the best epoch: 120.\n======================================================================\n[STEP 10] ✅ Training completed!\n\n[FINAL] Training Summary:\n  Total epochs trained: 135\n  Best training loss: 0.046502\n  Best training accuracy: 0.984110\n  Final learning rate: 7.812500e-06\n\n[STEP 10] Saving final model...\n[STEP 10] ✅ Model saved as 'EEGNet-SD-Final.keras'\n\n[STEP 10] Plotting training history...\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 1200x500 with 2 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[STEP 10] ✅ Training history plot saved as 'final_model_training_history.png'\n\n[STEP 10] Evaluating model on training data...\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.9911 - loss: 0.0334\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n[STEP 10] Final training loss: 0.0190\n[STEP 10] Final training accuracy: 0.9953\n\n[STEP 10] Generating predictions on training data...\n\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\n[STEP 10] Training Confusion Matrix:\n[[11704    96]\n [   14 11786]]\n\n[FINAL] Training Metrics:\n  Accuracy:    0.9953\n  Precision:   0.9919\n  Recall:      0.9988\n  Specificity: 0.9919\n  F1 Score:    0.9954\n  AUC:         0.9995\n  TN=11704, FP=96, FN=14, TP=11786\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 600x500 with 2 Axes>",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkYAAAHqCAYAAADh64FkAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAShZJREFUeJzt3Xt8j/X/x/HnZ7Z9NmNz3KkchsIQmdJyCsuUMJRENRLllGPiJ4fQdxlyDJ2+5utQUt8kShahWA5zSE4R0sEmDYvYbLt+f/TZ59vHho1rm6se9+/tun3b+3pf1/W+Pp/NXnu93u/rYzMMwxAAAADkVtQDAAAAuFkQGAEAADgQGAEAADgQGAEAADgQGAEAADgQGAEAADgQGAEAADgQGAEAADgQGAEAADgQGFnYsWPHZLPZFBcXV6DXqVy5srp3716g17gR3bt3V+XKla/r2Pvuu0/33XefqeMpDDabTePGjbuuY2/297MgZWRkaPjw4apQoYLc3NwUFRVl+jWs+j1VUOLi4mSz2XTs2LGiHgqQJwRGN7Hsf1By20aMGFHUw8she2xPP/10rvtHjRrl7HPq1KlCHl3Bu9r79dfteoO4v4uLFy9q2rRpatiwofz8/OTl5aXbb79d/fv313fffVeg1/73v/+tyZMn6+GHH9aCBQs0ePDgAr1eYVq/fr3ze2zRokW59mnUqJFsNptq1659XdeYM2dOgf8hBhQ196IeAK5t/PjxCgkJcWmrXbu2KlWqpAsXLsjDw6OIRpaTl5eXPvjgA82ZM0eenp4u+9555x15eXnp4sWLRTS6gtW0aVMtXLjQpe3pp5/W3Xffrd69ezvbSpQoccPXunDhgtzdr+/H9+DBg3JzK5q/iU6dOqXWrVsrMTFRDz30kLp27aoSJUro4MGDevfdd/XGG28oPT29wK6/bt063XLLLZo2bVqBXWPNmjUFdu688PLy0pIlS/T444+7tB87dkybN2+Wl5fXdZ97zpw5KleuXL4yjk888YS6dOkiu91+3dcFChOBkQU88MADatCgQa77buQfuYLQunVrrVixQp9++qnat2/vbN+8ebOOHj2qTp066YMPPijCERacKlWqqEqVKi5tzz77rKpUqZLjl9RfZWRkKCsrK0cgeTU38r4X5S+o7t27a+fOnXr//ffVqVMnl30TJkzQqFGjCvT6J0+eVKlSpQr0Gvl5HwvCgw8+qBUrVujUqVMqV66cs33JkiUKCAjQbbfdptOnTxf4OM6fPy8fHx8VK1ZMxYoVK/DrAWahlGZhuc0x6t69u0qUKKGff/5ZUVFRKlGihMqXL69hw4YpMzPT5fgpU6bo3nvvVdmyZeXt7a2wsDC9//77NzSmW265RU2bNtWSJUtc2hcvXqw6depcMYW/bNkyhYWFydvbW+XKldPjjz+un3/+OUe/5cuXq3bt2vLy8lLt2rX14Ycf5nq+rKwsTZ8+XbVq1ZKXl5cCAgL0zDPPFMovhKvJfs+mTJmi6dOnq2rVqrLb7dq3b5/S09M1ZswYhYWFyc/PTz4+PmrSpIm++OKLHOe5fI7RuHHjZLPZdPjwYXXv3l2lSpWSn5+fevTooT/++MPl2MvnGGWXADdt2qQhQ4aofPny8vHxUYcOHfTrr7+6HJuVlaVx48YpODhYxYsXV/PmzbVv3748zVvasmWLVq1apZ49e+YIiqQ/A7YpU6a4tK1bt05NmjSRj4+PSpUqpfbt22v//v0uffJy79mv+xdffKG9e/c6S07r1693lqDWr1/vct7cfr6SkpLUo0cP3XrrrbLb7QoKClL79u1d5s/kNsfo5MmT6tmzpwICAuTl5aW6detqwYIFuV5vypQpeuONN5zfG3fddZe2bdt21df2r9q3by+73a5ly5a5tC9ZskSdO3fONUiZP3++WrRoIX9/f9ntdoWGhmru3LkufSpXrqy9e/dqw4YNztcv+z6zv4c2bNigvn37yt/fX7feeqvLvuzXaN26dXJzc9OYMWNyjM9ms+W4LlDYyBhZwNmzZ3PMyfnrX4KXy8zMVGRkpBo2bKgpU6bo888/19SpU1W1alX16dPH2W/GjBlq166dunXrpvT0dL377rt65JFHtHLlSrVp0+a6x9u1a1cNHDhQ586dU4kSJZSRkaFly5ZpyJAhuZbR4uLi1KNHD911112KiYlRcnKyZsyYoU2bNmnnzp3Ov/DXrFmjTp06KTQ0VDExMfrtt9+cv6Qu98wzzzjP+9xzz+no0aOaPXu2du7cqU2bNhV5+XH+/Pm6ePGievfuLbvdrjJlyig1NVVvvfWWHnvsMfXq1Uu///673n77bUVGRmrr1q2qV6/eNc/buXNnhYSEKCYmRjt27NBbb70lf39/TZo06ZrHDhgwQKVLl9bYsWN17NgxTZ8+Xf3799fSpUudfUaOHKnY2Fi1bdtWkZGR2r17tyIjI/NUHl2xYoWkP0srefH555/rgQceUJUqVTRu3DhduHBBs2bNUqNGjbRjx44cc7Wudu/ly5fXwoUL9fLLL+vcuXOKiYmRJNWsWTNHoHU1nTp10t69ezVgwABVrlxZJ0+eVHx8vI4fP37FuWMXLlzQfffdp8OHD6t///4KCQnRsmXL1L17d505c0YDBw506b9kyRL9/vvveuaZZ2Sz2RQbG6uOHTvqyJEjefq+LV68uNq3b6933nnH+fO+e/du7d27V2+99Za++eabHMfMnTtXtWrVUrt27eTu7q6PP/5Yffv2VVZWlvr16ydJmj59ugYMGKASJUo4M3sBAQEu5+nbt6/Kly+vMWPG6Pz587mOr0WLFurbt69iYmIUFRWl+vXr68SJExowYIAiIiL07LPPXvMegQJl4KY1f/58Q1Kum2EYxtGjRw1Jxvz5853HREdHG5KM8ePHu5zrzjvvNMLCwlza/vjjD5ev09PTjdq1axstWrRwaa9UqZIRHR19zfFKMvr162ekpKQYnp6exsKFCw3DMIxVq1YZNpvNOHbsmDF27FhDkvHrr786r+nv72/Url3buHDhgvNcK1euNCQZY8aMcbbVq1fPCAoKMs6cOeNsW7NmjSHJqFSpkrPtyy+/NCQZixcvdhnf6tWrc7Q3a9bMaNas2TXv7Xr5+Pi4vHbZ75mvr69x8uRJl74ZGRlGWlqaS9vp06eNgIAA46mnnnJpl2SMHTvW+XX263p5vw4dOhhly5Z1abv8/cz+PouIiDCysrKc7YMHDzaKFSvmfL2TkpIMd3d3IyoqyuV848aNMyRd83ukQ4cOhiTj9OnTV+2XrV69eoa/v7/x22+/Odt2795tuLm5GU8++aSzLT/33qxZM6NWrVoubV988YUhyfjiiy9c2i//+Tp9+rQhyZg8efJVx33599T06dMNScaiRYucbenp6UZ4eLhRokQJIzU11eV6ZcuWNVJSUpx9P/roI0OS8fHHH1/1utn3sWzZMmPlypWGzWYzjh8/bhiGYTz//PNGlSpVrvgaXP5vgWEYRmRkpPOYbLVq1cr15yX7e6hx48ZGRkZGrvuOHj3qbDt//rxRrVo1o1atWsbFixeNNm3aGL6+vsYPP/xw1XsECgOlNAt47bXXFB8f77Jdy+V/dTVp0kRHjhxxafP29nb+9+nTp3X27Fk1adJEO3bsuKHxli5dWq1bt9Y777wj6c+/gO+9915VqlQpR9/t27fr5MmT6tu3r8u8mTZt2qhGjRpatWqVJOnEiRPatWuXoqOj5efn5+x3//33KzQ01OWcy5Ytk5+fn+6//36dOnXKuYWFhalEiRK5lqYKW6dOnVS+fHmXtmLFijnnp2RlZSklJUUZGRlq0KBBnt+T3N733377Tampqdc8tnfv3rLZbC7HZmZm6ocffpAkrV27VhkZGerbt6/LcQMGDMjT2LLHULJkyWv2zX6/u3fvrjJlyjjb77jjDt1///365JNPchxzI/eeF97e3vL09NT69evzVZL95JNPFBgYqMcee8zZ5uHhoeeee07nzp3Thg0bXPo/+uijKl26tPPrJk2aSFKOn9+radWqlcqUKaN3331XhmHo3Xffdbn+5f76b0F2hrpZs2Y6cuSIzp49m+fr9urVK0/ziYoXL664uDjt379fTZs21apVqzRt2jRVrFgxz9cCCgqlNAu4++67rzj5OjdeXl45fumWLl06xz/mK1eu1MSJE7Vr1y6lpaU52//6y/F6de3aVU888YSOHz+u5cuXKzY2Ntd+2b90q1evnmNfjRo19NVXX7n0u+2223L0q169ukvgcOjQIZ09e1b+/v65XvPkyZP5upeUlBSXlVLe3t4uwdn1uHyVYbYFCxZo6tSpOnDggC5dunTN/pe7/BdL9i/Y06dPy9fX97qPlf73HlSrVs2lX5kyZVx+kV9J9vV///33a06Avtr3Rc2aNfXZZ585J/fmZfzXuve8sNvtmjRpkoYOHaqAgADdc889euihh/Tkk08qMDDwqvdy22235VgJWLNmTef+v7rW+5AXHh4eeuSRR7RkyRLdfffd+vHHH9W1a9cr9t+0aZPGjh2rhISEHHPSzp49m+fv97x+n0p/PjqgT58+eu211xQZGamnnnoqz8cCBYmM0d9QXv5i+/LLL9WuXTt5eXlpzpw5+uSTTxQfH6+uXbvKMIwbHkO7du1kt9sVHR2ttLQ0de7c+YbPmVdZWVny9/fPkWXL3saPH5+v83Xs2FFBQUHO7fI5Idfjr3+hZ1u0aJG6d++uqlWr6u2339bq1asVHx+vFi1aKCsrK0/nvdJ7n5f39EaOzYsaNWpIkvbs2WPK+S53veO/0h8Cly9WkKRBgwbpu+++U0xMjLy8vDR69GjVrFlTO3fuzP+Ar8Cs96Fr167atWuXxo0bp7p16+bIrGb7/vvv1bJlS506dUqvvvqqVq1apfj4eOcznvL6vSfl/n19JWlpac4J799//32OgAwoKmSM/qE++OADeXl56bPPPnNZvj1//nxTzu/t7a2oqCgtWrRIDzzwwBUni2eX1w4ePKgWLVq47Dt48KBzf/b/Hzp0KMc5Dh486PJ11apV9fnnn6tRo0b5+of6SqZOnery13pwcPANnzM377//vqpUqaL//ve/Lr+sx44dWyDXy6/s9+Dw4cMumYHffvstT9mMtm3bKiYmRosWLXKWh651rcvfW0k6cOCAypUr55ItuhHZGZkzZ864tF+eyclWtWpVDR06VEOHDtWhQ4dUr149TZ069YoPVaxUqZK++eYbZWVluWSNDhw44NxfEBo3bqyKFStq/fr1V518//HHHystLU0rVqxwyVZdaTWkWcaOHav9+/drypQpeuGFFzRixAjNnDnTtPMD14uM0T9UsWLFZLPZXP4qPnbsmJYvX27aNYYNG6axY8dq9OjRV+zToEED+fv7a968eS7lvE8//VT79+93ro4LCgpSvXr1tGDBApc5D/Hx8dq3b5/LOTt37qzMzExNmDAhx/UyMjJy/AK8lrCwMEVERDi3K/3lfaOyMwV/zQxs2bJFCQkJBXK9/GrZsqXc3d1zLKeePXt2no4PDw9X69at9dZbb+X6fZaenq5hw4ZJcn2///p+ffvtt1qzZo0efPDB676Py1WqVEnFihXTxo0bXdrnzJnj8vUff/yRY/Vd1apVVbJkSZfv3cs9+OCDSkpKclndl5GRoVmzZqlEiRJq1qyZCXeRk81m08yZMzV27NirrgTM7fvu7Nmzuf6R5OPjk++fn9xs2bJFU6ZM0aBBgzR06FA9//zzmj17do75VkBRIGP0D9WmTRu9+uqrat26tbp27aqTJ0/qtddeU7Vq1XJdzns96tatq7p16161j4eHhyZNmqQePXqoWbNmeuyxx5zL9StXruzykQ0xMTFq06aNGjdurKeeekopKSmaNWuWatWqpXPnzjn7NWvWTM8884xiYmK0a9cutWrVSh4eHjp06JCWLVumGTNm6OGHHzblHs300EMP6b///a86dOigNm3a6OjRo5o3b55CQ0Nd7q+oBAQEaODAgZo6daratWun1q1ba/fu3fr0009Vrly5PGUT/vOf/6hVq1bq2LGj2rZtq5YtW8rHx0eHDh3Su+++qxMnTjifZTR58mQ98MADCg8PV8+ePZ3L9f38/K77c+Jy4+fnp0ceeUSzZs2SzWZT1apVtXLlyhxz0b777ju1bNlSnTt3VmhoqNzd3fXhhx8qOTlZXbp0ueL5e/furddff13du3dXYmKiKleurPfff1+bNm3S9OnT8zQZ/Xq1b9/e5UGruWnVqpU8PT3Vtm1bPfPMMzp37pzefPNN+fv768SJEy59w8LCNHfuXE2cOFHVqlWTv79/jkzvtVy8eFHR0dG67bbb9PLLL0uSXnrpJX388cfq0aOH9uzZY1o2ELgeBEb/UC1atNDbb7+tV155RYMGDVJISIgmTZqkY8eOmRYY5VX37t1VvHhxvfLKK3rhhRecDxecNGmSyyTd1q1ba9myZXrxxRc1cuRIVa1aVfPnz9dHH32U4+F88+bNU1hYmF5//XX93//9n9zd3VW5cmU9/vjjatSoUaHeX151795dSUlJev311/XZZ58pNDRUixYt0rJly3LcX1GZNGmSihcvrjfffFOff/65wsPDtWbNGjVu3DhPT+MuX768Nm/erDlz5mjp0qUaNWqU0tPTValSJbVr185l/lZERIRWr16tsWPHasyYMfLw8FCzZs00adKkfE3yzYtZs2bp0qVLmjdvnux2uzp37qzJkye7PJC0QoUKeuyxx7R27VotXLhQ7u7uqlGjht57771cH1iZzdvbW+vXr9eIESO0YMECpaamqnr16po/f/5N8WG+1atX1/vvv68XX3xRw4YNU2BgoPr06aPy5cvnmBA9ZswY/fDDD4qNjdXvv/+uZs2a5Tsw+r//+z8dPnzY5eNJPD09tWDBAt1zzz16/vnnc2TrgMJkM8yaWQngH+nMmTMqXbq0Jk6cWOAf6QEABY05RgDy7MKFCznapk+fLkk5PgYDAKyIUhqAPFu6dKni4uL04IMPqkSJEvrqq6/0zjvvqFWrVjdtiRIA8oPACECe3XHHHXJ3d1dsbKxSU1OdE7InTpxY1EMDAFMwxwgAAMCBOUYAAAAOBEYAAAAOBEYAAAAOf8vJ1971nyvqIQCWl7KFz60CboS3RyFd587+pp/zws68fdTP3xEZIwAAAIe/ZcYIAIB/DBs5DjMRGAEAYGV5+ABn5B1hJgAAgAMZIwAArIxSmql4NQEAABzIGAEAYGXMMTIVgREAAFZGKc1UvJoAAAAOZIwAALAySmmmIjACAMDKKKWZilcTAADAgYwRAABWRinNVGSMAAAAHMgYAQBgZcwxMhWBEQAAVkYpzVSEmQAAAA5kjAAAsDJKaaYiMAIAwMoopZmKMBMAAMCBjBEAAFZGKc1UBEYAAFgZgZGpeDUBAAAcyBgBAGBlbky+NhMZIwAAAAcyRgAAWBlzjExFYAQAgJXxHCNTEWYCAAA4kDECAMDKKKWZisAIAAAro5RmKsJMAAAABzJGAABYGaU0U/FqAgAAOJAxAgDAyphjZCoCIwAArIxSmql4NQEAABzIGAEAYGWU0kxFYAQAgJVRSjMVryYAAIADGSMAAKyMUpqpCIwAALAySmmm4tUEAABwIGMEAICVkTEyFa8mAACAAxkjAACsjMnXpiIwAgDAyiilmYpXEwAAwIGMEQAAVkYpzVQERgAAWBmlNFPxagIAADiQMQIAwMoopZmKwAgAAAuzERiZilIaAACAAxkjAAAsjIyRucgYAQAAOJAxAgDAykgYmYqMEQAAFmaz2Uzf8mPjxo1q27atgoODZbPZtHz5cpf9hmFozJgxCgoKkre3tyIiInTo0CGXPikpKerWrZt8fX1VqlQp9ezZU+fOnXPp880336hJkyby8vJShQoVFBsbm2Msy5YtU40aNeTl5aU6derok08+yde9SARGAADgBpw/f15169bVa6+9luv+2NhYzZw5U/PmzdOWLVvk4+OjyMhIXbx40dmnW7du2rt3r+Lj47Vy5Upt3LhRvXv3du5PTU1Vq1atVKlSJSUmJmry5MkaN26c3njjDWefzZs367HHHlPPnj21c+dORUVFKSoqSt9++22+7sdmGIaRz9fgpudd/7miHgJgeSlbZhb1EABL8/YonOuUfHSB6ef8fWn0dR1ns9n04YcfKioqStKf2aLg4GANHTpUw4YNkySdPXtWAQEBiouLU5cuXbR//36FhoZq27ZtatCggSRp9erVevDBB/XTTz8pODhYc+fO1ahRo5SUlCRPT09J0ogRI7R8+XIdOHBAkvToo4/q/PnzWrlypXM899xzj+rVq6d58+bl+R7IGAEAYGFFXUq7mqNHjyopKUkRERHONj8/PzVs2FAJCQmSpISEBJUqVcoZFElSRESE3NzctGXLFmefpk2bOoMiSYqMjNTBgwd1+vRpZ5+/Xie7T/Z18orJ1wAAwEVaWprS0tJc2ux2u+x2e77Ok5SUJEkKCAhwaQ8ICHDuS0pKkr+/v8t+d3d3lSlTxqVPSEhIjnNk7ytdurSSkpKuep28ImMEAICFFUTGKCYmRn5+fi5bTExMUd9qoSBjBAAAXIwcOVJDhgxxactvtkiSAgMDJUnJyckKCgpyticnJ6tevXrOPidPnnQ5LiMjQykpKc7jAwMDlZyc7NIn++tr9cnen1dkjAAAsDKb+Zvdbpevr6/Ldj2BUUhIiAIDA7V27VpnW2pqqrZs2aLw8HBJUnh4uM6cOaPExERnn3Xr1ikrK0sNGzZ09tm4caMuXbrk7BMfH6/q1aurdOnSzj5/vU52n+zr5BWBEQAAFlbUk6/PnTunXbt2adeuXZL+nHC9a9cuHT9+XDabTYMGDdLEiRO1YsUK7dmzR08++aSCg4OdK9dq1qyp1q1bq1evXtq6das2bdqk/v37q0uXLgoODpYkde3aVZ6enurZs6f27t2rpUuXasaMGS5ZrYEDB2r16tWaOnWqDhw4oHHjxmn79u3q379/vu6HUhoAALhu27dvV/PmzZ1fZwcr0dHRiouL0/Dhw3X+/Hn17t1bZ86cUePGjbV69Wp5eXk5j1m8eLH69++vli1bys3NTZ06ddLMmf97ZIifn5/WrFmjfv36KSwsTOXKldOYMWNcnnV07733asmSJXrxxRf1f//3f7rtttu0fPly1a5dO1/3w3OMAOSK5xgBN6awnmNU+vHFpp/z9KJupp/TKsgYAQBgYWY+dwjMMQIAAHAiYwQAgIWRMTIXgREAAFZGXGQqSmkAAAAOZIwAALAwSmnmImMEAADgQMYIAAALI2NkLgIjAAAsjMDIXJTSAAAAHMgYAQBgZSSMTEVgBACAhVFKMxelNAAAAAcyRgAAWBgZI3MRGAEAYGEERuailAYAAOBAxggAAAsjY2QuMkYAAAAOZIwAALAyEkamIjACAMDCKKWZi1IaAACAAxkjAAAsjIyRuQiMAACwMAIjc1FKAwAAcCBjBACAlZEwMhUZIwAAAAcyRgAAWBhzjMxFYIQ8aVS/qgY/2VL1a1ZQUHk/dR7ypj5ev8e5v32LO/R0p8a6s2YFlS3lo4ZdJumb73527q8YVEYHV43L9dzdhv9b//18lySpQmBpzRjZWc0a3KZzF9K0eOVWjZ71sTIzs3IcF143RGvefE57vz+hex6LNfV+gZvB+fPn9NqsGfpi7edKSflN1WuEaviI/1PtOnc4+xz5/nvNmDZZidu3KSMzU1WqVNXU6bMUFBRchCNHYSIwMheBEfLEx8tTe777Wf/56Gstnfp0jv3Fve3avOuIPojfqbljHsux/6fk06p8/yiXtqc6NtLgJ1vos037JElubjb9d8YzSv4tVc17TFNgOV+9NeEJXcrI1NjZK12O9SvhrbfGP6Evtn0n/zIlTbxT4Obx0pgXdfjwIU2MiVV5f3+t+niFnu3VQx989IkCAgL04/Hj6vFkV0V17KQ+/Z6Tj08Jff/9Idk97UU9dMCyCIyQJ2s279eazfuvuP+dVdsk/ZkZyk1WlqHk3353aWvX/A59EL9T5y+kS5Ii7qmhmlUC1abPazqZ8ru++e5njZ+zShOfa6eJ8z7VpYxM57GzRnXW0tXblZllqO19dW709oCbzsWLF7X28zWaNnOOwhrcJUnq02+ANm74QsuWLlH/5wZr9sxpatykqQYPHe48rkLFikU1ZBQRMkbmYvI1isSdNSuoXo1btWD51862hneE6NvDv+hkyv8CqPiE/fIr6a3QqkHOtifaNVTILeX08hurC3XMQGHKzMxQZmam7HbX7I/dbtfOHTuUlZWlLzeuV6XKldWnd081bxquxx97ROvWfl5EI0ZRsdlspm//ZEUaGJ06dUqxsbHq0KGDwsPDFR4erg4dOmjy5Mn69ddfi3JoKGDR7e/R/iNJ+vqbo862gHIlXYIiSc6vA8r+WS6rWqG8Jgxoqx4v/ifXeUfA34WPTwndUfdOvTFvjk6eTFZmZqZWffyRvtm9S6dOnVRKym/6448/9O+339S9jZto7hv/VouW92vooP7avm1rUQ8fsKwiC4y2bdum22+/XTNnzpSfn5+aNm2qpk2bys/PTzNnzlSNGjW0ffv2a54nLS1NqampLpuRlXnN41B0vOweevSBMC1YnpCv49zcbFrwryc1cd6nOnycwBl/fy/HxEoy1KpFU91dv46WLF6o1g+0kZvNTVlZf/5hcF/zlnriye6qUaOmnnq6t5o2u0/vv/du0Q4chctWANs/WJHNMRowYIAeeeQRzZs3L0fazjAMPfvssxowYIASEq7+yzMmJkYvvfSSS1uxwLvlEdTQ9DHDHB0i6qm4l6cWr9zm0p586nc1qFXJpS17YnXyb7+rZHEvhdWqpLrVb9W0Fx6W9Gew5Obmpt+3TtND/eZow7ZDhXMTQCGoULGi3o5bpAt//KFz58+pfHl/DR86SLfcWkGlS5eWu7u7qlat6nJMSJWq2rkjsYhGjKLwTy99ma3IAqPdu3crLi4u1zfUZrNp8ODBuvPOO695npEjR2rIkCEubf5NR5o2Tpive/t7tGrDtzp15pxL+5ZvjuqFnq1UvnQJ/Xr6z30t76mhs79f0P4jSbqUkamwR2Jcjun9SGPdd9ft6jr83zr282+Fdg9AYfIuXlzexYsr9exZbd78lQYNeV4eHp4KrVVHx44eden7w7FjCgq+pYhGClhfkQVGgYGB2rp1q2rUqJHr/q1btyogIOCa57Hb7TkmJ9rcipkyRvyPj7enqlYo7/y68i1ldcftt+h06h/6Mem0SvsWV4XA0goq7ydJur2yvyQp+bdUl9VoVSqUU+P6VRX13Os5rvH51we0/0iS3p74hEZN/0gB5Xw1tm8bvb7sS6VfypAk7fv+hMsxv54+p4vpl3K0A38Hmzd9KcMwVLlyiI4fP65pU2MVElJF7aM6SpK69+ip4cMGq36Du3TX3Q21+asvtXHDF3pr/n+KeOQoTGSMzFVkgdGwYcPUu3dvJSYmqmXLls4gKDk5WWvXrtWbb76pKVOmFNXwcJn6oRW15s3nnF/HDv3zH+aFK7ao97jFatOstt586XHn/oWv9JAkTXz9U738+qfO9uj29+jn5DP6POFAjmtkZRnqNOh1zRjZWevjhuj8xXQt/niLxs/9pKBuC7ip/f7775o1/VUlJyfJz6+UWt7fSv2fGywPDw9JUouI+/XimHF6+603FBszUZUqh2jKtJm6s36DIh45YF02wzCMorr40qVLNW3aNCUmJioz888J08WKFVNYWJiGDBmizp07X9d5ves/d+1OAK4qZcvMoh4CYGneHoVznWrDPr12p3w6POUB089pFUX6gMdHH31Ujz76qC5duqRTp05JksqVK+f8awgAAFwdpTRz3RRPvvbw8FBQUNC1OwIAABSgmyIwAgAA14eEkbkIjAAAsDBKaebis9IAAAAcyBgBAGBhJIzMRWAEAICFubkRGZmJUhoAAIADGSMAACyMUpq5yBgBAAA4kDECAMDCWK5vLgIjAAAsjLjIXJTSAAAAHMgYAQBgYZTSzEVgBACAhREYmYtSGgAAgAMZIwAALIyEkbnIGAEAADiQMQIAwMKYY2QuAiMAACyMuMhclNIAAMB1yczM1OjRoxUSEiJvb29VrVpVEyZMkGEYzj6GYWjMmDEKCgqSt7e3IiIidOjQIZfzpKSkqFu3bvL19VWpUqXUs2dPnTt3zqXPN998oyZNmsjLy0sVKlRQbGxsgdwTgREAABZms9lM3/Jq0qRJmjt3rmbPnq39+/dr0qRJio2N1axZs5x9YmNjNXPmTM2bN09btmyRj4+PIiMjdfHiRWefbt26ae/evYqPj9fKlSu1ceNG9e7d27k/NTVVrVq1UqVKlZSYmKjJkydr3LhxeuONN8x5Ef/CZvw1rPub8K7/XFEPAbC8lC0zi3oIgKV5exTOdRpM/ML0c25/sXme+j300EMKCAjQ22+/7Wzr1KmTvL29tWjRIhmGoeDgYA0dOlTDhg2TJJ09e1YBAQGKi4tTly5dtH//foWGhmrbtm1q0KCBJGn16tV68MEH9dNPPyk4OFhz587VqFGjlJSUJE9PT0nSiBEjtHz5ch04cMDUeydjBAAAXKSlpSk1NdVlS0tLy9Hv3nvv1dq1a/Xdd99Jknbv3q2vvvpKDzzwgCTp6NGjSkpKUkREhPMYPz8/NWzYUAkJCZKkhIQElSpVyhkUSVJERITc3Ny0ZcsWZ5+mTZs6gyJJioyM1MGDB3X69GlT753ACAAACyuIUlpMTIz8/PxctpiYmBzXHjFihLp06aIaNWrIw8NDd955pwYNGqRu3bpJkpKSkiRJAQEBLscFBAQ49yUlJcnf399lv7u7u8qUKePSJ7dz/PUaZmFVGgAAFlYQq9JGjhypIUOGuLTZ7fYc/d577z0tXrxYS5YsUa1atbRr1y4NGjRIwcHBio6ONn9ghYDACAAAuLDb7bkGQpd7/vnnnVkjSapTp45++OEHxcTEKDo6WoGBgZKk5ORkBQUFOY9LTk5WvXr1JEmBgYE6efKky3kzMjKUkpLiPD4wMFDJyckufbK/zu5jFkppAABYWFGuSvvjjz/k5uYaShQrVkxZWVmSpJCQEAUGBmrt2rXO/ampqdqyZYvCw8MlSeHh4Tpz5owSExOdfdatW6esrCw1bNjQ2Wfjxo26dOmSs098fLyqV6+u0qVL5/9FuwoCIwAAcF3atm2rl19+WatWrdKxY8f04Ycf6tVXX1WHDh0k/Rm0DRo0SBMnTtSKFSu0Z88ePfnkkwoODlZUVJQkqWbNmmrdurV69eqlrVu3atOmTerfv7+6dOmi4OBgSVLXrl3l6empnj17au/evVq6dKlmzJiRo9xnBkppAABYWFE++XrWrFkaPXq0+vbtq5MnTyo4OFjPPPOMxowZ4+wzfPhwnT9/Xr1799aZM2fUuHFjrV69Wl5eXs4+ixcvVv/+/dWyZUu5ubmpU6dOmjnzf48M8fPz05o1a9SvXz+FhYWpXLlyGjNmjMuzjszCc4wA5IrnGAE3prCeYxQ+aaPp50x4oanp57QKSmkAAAAOlNIAALAwPkTWXARGAABYWH5WkeHaKKUBAAA4kDECAMDCSBiZi4wRAACAAxkjAAAsjDlG5iIwAgDAwgiMzEUpDQAAwIGMEQAAFkbCyFwERgAAWBilNHNRSgMAAHAgYwQAgIWRMDIXgREAABZGKc1clNIAAAAcyBgBAGBhJIzMRcYIAADAgYwRAAAW5kbKyFQERgAAWBhxkbkopQEAADiQMQIAwMJYrm8uAiMAACzMjbjIVJTSAAAAHMgYAQBgYZTSzEVgBACAhREXmYtSGgAAgAMZIwAALMwmUkZmImMEAADgQMYIAAALY7m+uQiMAACwMFalmYtSGgAAgAMZIwAALIyEkbkIjAAAsDA3IiNTUUoDAABwIGMEAICFkTAyFxkjAAAABzJGAABYGMv1zUVgBACAhREXmYtSGgAAgAMZIwAALIzl+uYiMAIAwMIIi8xFKQ0AAMCBjBEAABbGqjRzERgBAGBhbsRFpqKUBgAA4EDGCAAAC6OUZi4yRgAAAA5kjAAAsDASRuYiMAIAwMIopZmLUhoAAIADGSMAACyM5frmIjACAMDCKKWZ67pKaV9++aUef/xxhYeH6+eff5YkLVy4UF999ZWpgwMAAChM+Q6MPvjgA0VGRsrb21s7d+5UWlqaJOns2bP617/+ZfoAAQDAldkKYPsny3dgNHHiRM2bN09vvvmmPDw8nO2NGjXSjh07TB0cAAC4OjebzfTtnyzfgdHBgwfVtGnTHO1+fn46c+aMGWMCAAAoEvkOjAIDA3X48OEc7V999ZWqVKliyqAAAEDe2Gzmb/9k+Q6MevXqpYEDB2rLli2y2Wz65ZdftHjxYg0bNkx9+vQpiDECAAAUinwHRiNGjFDXrl3VsmVLnTt3Tk2bNtXTTz+tZ555RgMGDCiIMQIAgCuw2Wymb/nx888/6/HHH1fZsmXl7e2tOnXqaPv27c79hmFozJgxCgoKkre3tyIiInTo0CGXc6SkpKhbt27y9fVVqVKl1LNnT507d86lzzfffKMmTZrIy8tLFSpUUGxs7PW/aFeR78DIZrNp1KhRSklJ0bfffquvv/5av/76qyZMmFAQ4wMAAFdRlKW006dPq1GjRvLw8NCnn36qffv2aerUqSpdurSzT2xsrGbOnKl58+Zpy5Yt8vHxUWRkpC5evOjs061bN+3du1fx8fFauXKlNm7cqN69ezv3p6amqlWrVqpUqZISExM1efJkjRs3Tm+88YYpr+Ff2QzDMEw/axHzrv9cUQ8BsLyULTOLegiApXl7XLuPGZ55f6/p53z94Vp56jdixAht2rRJX375Za77DcNQcHCwhg4dqmHDhkn68/E+AQEBiouLU5cuXbR//36FhoZq27ZtatCggSRp9erVevDBB/XTTz8pODhYc+fO1ahRo5SUlCRPT0/ntZcvX64DBw6YcMf/k+8nXzdv3vyqabZ169bd0IAAAEDeFcTy+rS0NOdzCrPZ7XbZ7XaXthUrVigyMlKPPPKINmzYoFtuuUV9+/ZVr169JElHjx5VUlKSIiIinMf4+fmpYcOGSkhIUJcuXZSQkKBSpUo5gyJJioiIkJubm7Zs2aIOHTooISFBTZs2dQZFkhQZGalJkybp9OnTLhmqG5XvUlq9evVUt25d5xYaGqr09HTt2LFDderUMW1gAADg2gqilBYTEyM/Pz+XLSYmJse1jxw5orlz5+q2227TZ599pj59+ui5557TggULJElJSUmSpICAAJfjAgICnPuSkpLk7+/vst/d3V1lypRx6ZPbOf56DbPkO2M0bdq0XNvHjRuXY6IUAACwnpEjR2rIkCEubZdniyQpKytLDRo0cH7yxZ133qlvv/1W8+bNU3R0dKGM1WzX9VlpuXn88cf173//26zTAQCAPCiIVWl2u12+vr4uW26BUVBQkEJDQ13aatasqePHj0v689mHkpScnOzSJzk52bkvMDBQJ0+edNmfkZGhlJQUlz65neOv1zCLaYFRQkKCvLy8zDodAAC4yTVq1EgHDx50afvuu+9UqVIlSVJISIgCAwO1du1a5/7U1FRt2bJF4eHhkqTw8HCdOXNGiYmJzj7r1q1TVlaWGjZs6OyzceNGXbp0ydknPj5e1atXN3V+kXQdpbSOHTu6fG0Yhk6cOKHt27dr9OjRpg3sRpzeymoa4EaVvqt/UQ8BsLQLO2cXynVMy3Bch8GDB+vee+/Vv/71L3Xu3Flbt27VG2+84VxGb7PZNGjQIE2cOFG33XabQkJCNHr0aAUHBysqKkrSnxmm1q1bq1evXpo3b54uXbqk/v37q0uXLgoODpYkde3aVS+99JJ69uypF154Qd9++61mzJhxxek9NyLfgZGfn5/L125ubqpevbrGjx+vVq1amTYwAABwbfl9IKOZ7rrrLn344YcaOXKkxo8fr5CQEE2fPl3dunVz9hk+fLjOnz+v3r1768yZM2rcuLFWr17tUmVavHix+vfvr5YtW8rNzU2dOnXSzJn/S3L4+flpzZo16tevn8LCwlSuXDmNGTPG5VlHZsnXc4wyMzO1adMm1alTx/TUlZkuZhT1CADrI2ME3JjCyhg9t9zc5/hI0syoGqaf0yrylYErVqyYWrVqpTNnzhTQcAAAQH642czf/snyXZqsXbu2jhw5UhBjAQAA+URgZK58B0YTJ07UsGHDtHLlSp04cUKpqakuGwAAgFXlefL1+PHjNXToUD344IOSpHbt2rlM+DIMQzabTZmZmeaPEgAA5KooJ1//HeU5MHrppZf07LPP6osvvijI8QAAgHz4p5e+zJbnwCh78VqzZs0KbDAAAABFKV/PMSJdBwDAzYVfzebKV2B0++23XzM4SklJuaEBAQAAFJV8BUYvvfRSjidfAwCAouNGyshU+QqMunTpIn9//4IaCwAAyKei/Ky0v6M8v57MLwIAAH93+V6VBgAAbh7kLcyV58AoKyurIMcBAACuA3OMzEVpEgAAwCFfk68BAMDNhYSRuQiMAACwMD4SxFyU0gAAABzIGAEAYGFMvjYXGSMAAAAHMkYAAFgYCSNzERgBAGBhTL42F6U0AAAABzJGAABYmE2kjMxEYAQAgIVRSjMXpTQAAAAHMkYAAFgYGSNzkTECAABwIGMEAICF2XiQkakIjAAAsDBKaeailAYAAOBAxggAAAujkmYuAiMAACzMjcjIVJTSAAAAHMgYAQBgYUy+NheBEQAAFkYlzVyU0gAAABzIGAEAYGFuImVkJjJGAAAADmSMAACwMOYYmYvACAAAC2NVmrkopQEAADiQMQIAwMJ48rW5CIwAALAw4iJzUUoDAABwIGMEAICFUUozF4ERAAAWRlxkLkppAAAADmSMAACwMDIc5uL1BAAAcCBjBACAhdmYZGQqAiMAACyMsMhclNIAAAAcyBgBAGBhPMfIXARGAABYGGGRuSilAQAAOJAxAgDAwqikmYuMEQAAgAOBEQAAFmaz2Uzfrtcrr7wim82mQYMGOdsuXryofv36qWzZsipRooQ6deqk5ORkl+OOHz+uNm3aqHjx4vL399fzzz+vjIwMlz7r169X/fr1ZbfbVa1aNcXFxV33OK+GwAgAAAtzK4Dtemzbtk2vv/667rjjDpf2wYMH6+OPP9ayZcu0YcMG/fLLL+rYsaNzf2Zmptq0aaP09HRt3rxZCxYsUFxcnMaMGePsc/ToUbVp00bNmzfXrl27NGjQID399NP67LPPrnO0V0ZgBAAAbsi5c+fUrVs3vfnmmypdurSz/ezZs3r77bf16quvqkWLFgoLC9P8+fO1efNmff3115KkNWvWaN++fVq0aJHq1aunBx54QBMmTNBrr72m9PR0SdK8efMUEhKiqVOnqmbNmurfv78efvhhTZs2zfR7ITACAMDCboZSWr9+/dSmTRtFRES4tCcmJurSpUsu7TVq1FDFihWVkJAgSUpISFCdOnUUEBDg7BMZGanU1FTt3bvX2efyc0dGRjrPYSZWpQEAYGEFsSgtLS1NaWlpLm12u112uz1H33fffVc7duzQtm3bcuxLSkqSp6enSpUq5dIeEBCgpKQkZ5+/BkXZ+7P3Xa1PamqqLly4IG9v7/zd4FWQMQIAAC5iYmLk5+fnssXExOTo9+OPP2rgwIFavHixvLy8imCk5iMwAgDAwgqilDZy5EidPXvWZRs5cmSOaycmJurkyZOqX7++3N3d5e7urg0bNmjmzJlyd3dXQECA0tPTdebMGZfjkpOTFRgYKEkKDAzMsUot++tr9fH19TU1WyQRGAEAYGkFsSrNbrfL19fXZcutjNayZUvt2bNHu3btcm4NGjRQt27dnP/t4eGhtWvXOo85ePCgjh8/rvDwcElSeHi49uzZo5MnTzr7xMfHy9fXV6Ghoc4+fz1Hdp/sc5iJOUYAAOC6lCxZUrVr13Zp8/HxUdmyZZ3tPXv21JAhQ1SmTBn5+vpqwIABCg8P1z333CNJatWqlUJDQ/XEE08oNjZWSUlJevHFF9WvXz9nMPbss89q9uzZGj58uJ566imtW7dO7733nlatWmX6PREYAQBgYTfyQMbCMG3aNLm5ualTp05KS0tTZGSk5syZ49xfrFgxrVy5Un369FF4eLh8fHwUHR2t8ePHO/uEhIRo1apVGjx4sGbMmKFbb71Vb731liIjI00fr80wDMP0sxaxixnX7gPg6krf1b+ohwBY2oWdswvlOh9+k2T6OTvcEWj6Oa2CjBEAABZ2c+eLrIfACAAAC7vJK2mWw6o0AAAABzJGAABYmBvFNFMRGAEAYGGU0sxFKQ0AAMCBjBEAABZmo5RmKjJGAAAADmSMAACwMOYYmYvACAAAC2NVmrkopQEAADiQMQIAwMIopZmLwAgAAAsjMDIXpTQAAAAHMkYAAFgYzzEyF4ERAAAW5kZcZCpKaQAAAA5kjAAAsDBKaeYiYwQAAOBAxggAAAtjub65CIwAALAwSmnmopQGAADgQMYIAAALY7m+uQiMAACwMEpp5qKUhgKTuH2bBvR9VhH3NVbdWtW1bu3nV+w74aUxqluruhb9J67wBggUokb1q+r96c/oyJqXdWHnbLW97w6X/e1b1NXHc/rppy8m6cLO2brj9ltc9lcMKqMLO2fnunWMuNPZLyy0oj6ZN0AnNsbqlw2xWvFaP9W57FySNOiJlvpm+Rid2TJN3382UcN7RhbMjQMWQ8YIBebChT9UvXp1RXXspCED+1+x39rP47Vn926V9/cvxNEBhcvH26493/2s/3yUoKWv9s6xv7i3pzbv+l4fxO/Q3DHdcuz/Kfm0KkeMdGl7qlMjDX4yQp9t2uu4hqc+eq2fVm3Yo4ExS+VezE2j+7TRitf66bYHXlRGRpYkaerwh9XynhoaOe1DfXvoF5XxK67Svj4FcNcoDKxKMxeBEQpM4ybN1LhJs6v2SU5O1iv/mqC5b7ytAX2eKaSRAYVvzaZ9WrNp3xX3v7Nqm6Q/M0O5ycoylPzb7y5t7ZrX1QfxO3T+QrokqXpIoMqW8tGEuSv1U/IZSdLLr3+q7cv+TxWDyujIj6dUPSRAvR5uorBHXtahH05Kkn745bcbvT0UIeIic1FKQ5HJysrSqBHPq3uPnqpW7baiHg5gKXfWrKB6NSpowfIEZ9t3x5J16vQ5RUfdKw/3YvKye6h7VLj2HzmhH35JkSS1aVpHR38+pQeb1tb+leN0YNVLmjOmq0r7Fi+qWwFuKjd1YPTjjz/qqaeeKuphoIDMf/tNFXN3V9fHnyzqoQCWE+0IeL7efdTZdu6PNEX2mqHHHrxLp7+eplObpur+e2sqqv8cZWb+WUarfGs5VQwqo44Rd+rp0QvVa8wi3VmzgpZM7llUt4Ib5Gazmb79k93UgVFKSooWLFhw1T5paWlKTU112dLS0gpphLhe+/Z+q8UL/6MJL8fI9g//IQTyy8vuoUcfaOCSLcpunze2mxJ2H1GzJ6eoRY9Xte/7E/rvzD7ysntI+vOXqJfdQz1HL9Smnd/ry8RD6vPSYt13d3XdVol5fkCRzjFasWLFVfcfOXLkmueIiYnRSy+95NI2avRYvThm3I0MDQVsR+J2paT8ptYRzZ1tmZmZmjp5khYv/I8+jV9XhKMDbm4dIuqpuJenFq/c6tL+6AMNVDG4jJpFT5VhGJKk6JFxOrExVm3vu0PLPktU0qmzunQpU4ePn3Qed+BosiSpQmAZ57wjWAd/WpqrSAOjqKgo2Ww25w9wbq6VTRg5cqSGDBni0mYUs5syPhSch9q1V8Pwe13a+vTuqYfatldUh45FNCrAGrpH3atVG/bo1OlzLu3FvTyVlWW4/JuaZRgyDDnLIwm7jsjDo5hCbi2noz+dkiRnpuj4iZRCugOYisjIVEUaGAUFBWnOnDlq3759rvt37dqlsLCwq57DbrfLbncNhC5mmDZE3IA/zp/X8ePHnV///NNPOrB/v/z8/BQUHKxSpUq79Pdw91C5cuVUOaRKYQ8VKHA+3p6qWqG88+vKt5TVHbffotOpf+jHpNMq7VtcFQJLK8jfT5J0e+UASVLyb6kuq9GqVCinxvWrKmrA3BzXWPv1Af1rUJSmj+ysue9ukJvNpmE9WikjM1Mbtn8nSVq35aB27Duu18d10/OTP5Cbm03TR3TW5wn7XbJIwD9VkQZGYWFhSkxMvGJgdK1sEm5ue/d+q6d7/G9i9ZTYGElSu/YdNOFfrxTVsIAiUT+0kta8NdD5deywTpKkhSu+Vu+xi9SmWR29Of4J5/6Fk/5ceDJx3id6+fVPnO3R7cP1c/IZfZ5wIMc1vjuWrE4DX9eoZx7Q+gVDlZVlaPeBn9S+3xwlnUqVJBmGoYcHva5XX3hE8W8P0vkL6VqzaZ9GvPrfArlvFDyefG0um1GEkceXX36p8+fPq3Xr1rnuP3/+vLZv365mza7+LJzLkTECblzpu678UE4A13Zh5+xCuc7WI2dNP+fdVfxMP6dVFGnGqEmTJlfd7+Pjk++gCAAA4Hrx5GsAACyMQpq5burnGAEAABQmMkYAAFgZKSNTERgBAGBhrEozF6U0AAAABzJGAABYGB83aS4CIwAALIy4yFyU0gAAABzIGAEAYGWkjExFYAQAgIWxKs1clNIAAAAcyBgBAGBhrEozFxkjAAAABzJGAABYGAkjcxEYAQBgZURGpqKUBgAA4EDGCAAAC2O5vrkIjAAAsDBWpZmLUhoAAIADGSMAACyMhJG5CIwAALAyIiNTUUoDAABwIDACAMDCbAXwv7yKiYnRXXfdpZIlS8rf319RUVE6ePCgS5+LFy+qX79+Klu2rEqUKKFOnTopOTnZpc/x48fVpk0bFS9eXP7+/nr++eeVkZHh0mf9+vWqX7++7Ha7qlWrpri4uOt+za6GwAgAAFyXDRs2qF+/fvr6668VHx+vS5cuqVWrVjp//ryzz+DBg/Xxxx9r2bJl2rBhg3755Rd17NjRuT8zM1Nt2rRRenq6Nm/erAULFiguLk5jxoxx9jl69KjatGmj5s2ba9euXRo0aJCefvppffbZZ6bfk80wDMP0sxaxixnX7gPg6krf1b+ohwBY2oWdswvlOvt+OX/tTvkUGuxzXcf9+uuv8vf314YNG9S0aVOdPXtW5cuX15IlS/Twww9Lkg4cOKCaNWsqISFB99xzjz799FM99NBD+uWXXxQQECBJmjdvnl544QX9+uuv8vT01AsvvKBVq1bp22+/dV6rS5cuOnPmjFavXn3jN/wXZIwAALAwWwFsaWlpSk1NddnS0tKuOZazZ89KksqUKSNJSkxM1KVLlxQREeHsU6NGDVWsWFEJCQmSpISEBNWpU8cZFElSZGSkUlNTtXfvXmefv54ju0/2OcxEYAQAAFzExMTIz8/PZYuJibnqMVlZWRo0aJAaNWqk2rVrS5KSkpLk6empUqVKufQNCAhQUlKSs89fg6Ls/dn7rtYnNTVVFy5cuO77zA3L9QEAsLICWK4/cuRIDRkyxKXNbrdf9Zh+/frp22+/1VdffWX+gAoRgREAABZWEJ+VZrfbrxkI/VX//v21cuVKbdy4UbfeequzPTAwUOnp6Tpz5oxL1ig5OVmBgYHOPlu3bnU5X/aqtb/2uXwlW3Jysnx9feXt7Z2ve7sWSmkAAOC6GIah/v3768MPP9S6desUEhLisj8sLEweHh5au3ats+3gwYM6fvy4wsPDJUnh4eHas2ePTp486ewTHx8vX19fhYaGOvv89RzZfbLPYSYyRgAAWFhRfohsv379tGTJEn300UcqWbKkc06Qn5+fvL295efnp549e2rIkCEqU6aMfH19NWDAAIWHh+uee+6RJLVq1UqhoaF64oknFBsbq6SkJL344ovq16+fM2v17LPPavbs2Ro+fLieeuoprVu3Tu+9955WrVpl+j2xXB9ArliuD9yYwlqufzDpD9PPWT2weJ762a4Qlc2fP1/du3eX9OcDHocOHap33nlHaWlpioyM1Jw5c5xlMkn64Ycf1KdPH61fv14+Pj6Kjo7WK6+8Inf3/+Vv1q9fr8GDB2vfvn269dZbNXr0aOc1zERgBCBXBEbAjSmswOi7AgiMbs9jYPR3RCkNAAAr40NkTcXkawAAAAcyRgAAWFhBLNf/JyMwAgDAwopyVdrfEaU0AAAABzJGAABYGAkjcxEYAQBgZURGpqKUBgAA4EDGCAAAC2NVmrnIGAEAADiQMQIAwMJYrm8uAiMAACyMuMhclNIAAAAcyBgBAGBlpIxMRWAEAICFsSrNXJTSAAAAHMgYAQBgYaxKMxeBEQAAFkZcZC5KaQAAAA5kjAAAsDBKaeYiYwQAAOBAxggAAEsjZWQmAiMAACyMUpq5KKUBAAA4kDECAMDCSBiZi8AIAAALo5RmLkppAAAADmSMAACwMD5E1lxkjAAAABzIGAEAYGUkjExFYAQAgIURF5mLUhoAAIADGSMAACyM5frmIjACAMDCWJVmLkppAAAADmSMAACwMhJGpiIwAgDAwoiLzEUpDQAAwIGMEQAAFsaqNHORMQIAAHAgYwQAgIWxXN9cBEYAAFgYpTRzUUoDAABwIDACAABwoJQGAICFUUozFxkjAAAABzJGAABYGKvSzEXGCAAAwIGMEQAAFsYcI3MRGAEAYGHEReailAYAAOBAxggAACsjZWQqAiMAACyMVWnmopQGAADgQMYIAAALY1WauQiMAACwMOIic1FKAwAAcCBjBACAlZEyMhUZIwAAAAcyRgAAWBjL9c1FYAQAgIWxKs1clNIAAAAcbIZhGEU9CPyzpKWlKSYmRiNHjpTdbi/q4QCWw88QUHAIjFDoUlNT5efnp7Nnz8rX17eohwNYDj9DQMGhlAYAAOBAYAQAAOBAYAQAAOBAYIRCZ7fbNXbsWCaNAteJnyGg4DD5GgAAwIGMEQAAgAOBEQAAgAOBEQAAgAOBEQrda6+9psqVK8vLy0sNGzbU1q1bi3pIgGVs3LhRbdu2VXBwsGw2m5YvX17UQwL+VgiMUKiWLl2qIUOGaOzYsdqxY4fq1q2ryMhInTx5sqiHBljC+fPnVbduXb322mtFPRTgb4lVaShUDRs21F133aXZs2dLkrKyslShQgUNGDBAI0aMKOLRAdZis9n04YcfKioqqqiHAvxtkDFCoUlPT1diYqIiIiKcbW5uboqIiFBCQkIRjgwAgD8RGKHQnDp1SpmZmQoICHBpDwgIUFJSUhGNCgCA/yEwAgAAcCAwQqEpV66cihUrpuTkZJf25ORkBQYGFtGoAAD4HwIjFBpPT0+FhYVp7dq1zrasrCytXbtW4eHhRTgyAAD+5F7UA8A/y5AhQxQdHa0GDRro7rvv1vTp03X+/Hn16NGjqIcGWMK5c+d0+PBh59dHjx7Vrl27VKZMGVWsWLEIRwb8PbBcH4Vu9uzZmjx5spKSklSvXj3NnDlTDRs2LOphAZawfv16NW/ePEd7dHS04uLiCn9AwN8MgREAAIADc4wAAAAcCIwAAAAcCIwAAAAcCIwAAAAcCIwAAAAcCIwAAAAcCIwAAAAcCIwAAAAcCIyAf7ju3bsrKirK+fV9992nQYMGFfo41q9fL5vNpjNnzhT6tQEgG4ERcJPq3r27bDabbDabPD09Va1aNY0fP14ZGRkFet3//ve/mjBhQp76EswA+LvhQ2SBm1jr1q01f/58paWl6ZNPPlG/fv3k4eGhkSNHuvRLT0+Xp6enKdcsU6aMKecBACsiYwTcxOx2uwIDA1WpUiX16dNHERERWrFihbP89fLLLys4OFjVq1eXJP3444/q3LmzSpUqpTJlyqh9+/Y6duyY83yZmZkaMmSISpUqpbJly2r48OG6/OMSLy+lpaWl6YUXXlCFChVkt9tVrVo1vf322zp27Jjzw0xLly4tm82m7t27S5KysrIUExOjkJAQeXt7q27dunr//fddrvPJJ5/o9ttvl7e3t5o3b+4yTgAoKgRGgIV4e3srPT1dkrR27VodPHhQ8fHxWrlypS5duqTIyEiVLFlSX375pTZt2qQSJUqodevWzmOmTp2quLg4/fvf/9ZXX32llJQUffjhh1e95pNPPql33nlHM2fO1P79+/X666+rRIkSqlChgj744ANJ0sGDB3XixAnNmDFDkhQTE6P//Oc/mjdvnvbu3avBgwfr8ccf14YNGyT9GcB17NhRbdu21a5du/T0009rxIgRBfWyAUDeGQBuStHR0Ub79u0NwzCMrKwsIz4+3rDb7cawYcOM6OhoIyAgwEhLS3P2X7hwoVG9enUjKyvL2ZaWlmZ4e3sbn332mWEYhhEUFGTExsY691+6dMm49dZbndcxDMNo1qyZMXDgQMMwDOPgwYOGJCM+Pj7XMX7xxReGJOP06dPOtosXLxrFixc3Nm/e7NK3Z8+exmOPPWYYhmGMHDnSCA0Nddn/wgsv5DgXABQ25hgBN7GVK1eqRIkSunTpkrKystS1a1eNGzdO/fr1U506dVzmFe3evVuHDx9WyZIlXc5x8eJFff/99zp79qxOnDihhg0bOve5u7urQYMGOcpp2Xbt2qVixYqpWbNmeR7z4cOH9ccff+j+++93aU9PT9edd94pSdq/f7/LOCQpPDw8z9cAgIJCYATcxJo3b665c+fK09NTwcHBcnf/34+sj4+PS99z584pLCxMixcvznGe8uXLX9f1vb29833MuXPnJEmrVq3SLbfc4rLPbrdf1zgAoLAQGAE3MR8fH1WrVi1PfevXr6+lS5fK399fvr6+ufYJCgrSli1b1LRpU0lSRkaGEhMTVb9+/Vz716lTR1lZWdqwYYMiIiJy7M/OWGVmZjrbQkNDZbfbdfz48StmmmrWrKkVK1a4tH399dfXvkkAKGBMvgb+Jrp166Zy5cqpffv2+vLLL3X06FGtX79ezz33nH766SdJ0sCBA/XKK69o+fLlOnDggPr27XvVZxBVrlxZ0dHReuqpp7R8+XLnOd977z1JUqVKlWSz2bRy5Ur9+uuvOnfunEqWLKlhw4Zp8ODBWrBggb7//nvt2LFDs2bN0oIFCyRJzz77rA4dOqTnn39eBw8e1JIlSxQXF1fQLxEAXBOBEfA3Ubx4cW3cuFEVK1ZUx44dVbNmTfXs2VMXL150ZpCGDh2qJ554QtHR0QoPD1fJkiXVoUOHq5537ty5evjhh9W3b1/VqFFDvXr10vnz5yVJt9xyi1566SWNGDFCAQEB6t+/vyRpwoQJGj16tGJiYlSzZk21bt1aq1atUkhIiCSpYsWK+uCDD7R8+XLVrVtX8+bN07/+9a8CfHUAIG9sxpVmXQIAAPzDkDECAABwIDACAABwIDACAABwIDACAABwIDACAABwIDACAABwIDACAABwIDACAABwIDACAABwIDACAABwIDACAABwIDACAABw+H++An/MxIya2gAAAABJRU5ErkJggg=="
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[STEP 10] ✅ Confusion matrix plot saved as 'final_model_confusion_matrix.png'\n\n[STEP 10] Detailed Classification Report:\n              precision    recall  f1-score   support\n\n     Class 0       1.00      0.99      1.00     11800\n     Class 1       0.99      1.00      1.00     11800\n\n    accuracy                           1.00     23600\n   macro avg       1.00      1.00      1.00     23600\nweighted avg       1.00      1.00      1.00     23600\n\n\n======================================================================\n[FINAL] ✅ All training completed successfully!\n[FINAL] Model saved as: EEGNet-SD-Final.keras\n[FINAL] Training log saved as: final_model_training_log.csv\n======================================================================\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1767959266777
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.10 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
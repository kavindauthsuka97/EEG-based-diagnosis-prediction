{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4b572c-0225-422f-8d84-10fb8b49d2c2",
   "metadata": {},
   "source": [
    "CELL 1 — Imports + seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9555a00-9b76-4e4e-84be-aac3699cf002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports loaded and seeds set.\n"
     ]
    }
   ],
   "source": [
    "# Operating system utilities\n",
    "import os\n",
    "\n",
    "# Array handling\n",
    "import numpy as np\n",
    "\n",
    "# Pickle for loading normalization stats\n",
    "import pickle\n",
    "\n",
    "# EEG preprocessing libraries\n",
    "import mne\n",
    "import pywt\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"✅ Imports loaded and seeds set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8898c39a-4931-4584-a1b4-465d83bd85c8",
   "metadata": {},
   "source": [
    "CELL 2 — Load trained model + normalization stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08364221-736a-4bb5-af25-c225104902fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\anaconda3\\envs\\DL\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ EEGViT model loaded successfully.\n",
      "✅ Training mean and std loaded.\n",
      "✅ Safe EEGViT model saved to:\n",
      "C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\Dermerzel\\SomnasNest\\Alzheimer\\model inference\\ad_eegvit-v1_SAFE.h5\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# EEGViT – SAFE INFERENCE SCRIPT (ALL-IN-ONE)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import tempfile\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PATHS\n",
    "# ============================================================\n",
    "\n",
    "MODEL_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\Dermerzel\\SomnasNest\\Alzheimer\\model inference\\EEGViT model\\ad_eegvit-v1.keras\"\n",
    "MEAN_PATH  = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\Dermerzel\\SomnasNest\\Alzheimer\\model inference\\train_mean.pkl\"\n",
    "STD_PATH   = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\Dermerzel\\SomnasNest\\Alzheimer\\model inference\\train_std.pkl\"\n",
    "SAFE_MODEL_PATH = r\"C:\\Users\\HP\\Desktop\\Jupyter Notebooks\\Dermerzel\\SomnasNest\\Alzheimer\\model inference\\ad_eegvit-v1_SAFE.h5\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TRANSFORMER BLOCK\n",
    "# ============================================================\n",
    "\n",
    "def transformer_block(x, dim, heads, dim_head, mlp_dim, dropout):\n",
    "    # --- Self-attention (PreNorm) ---\n",
    "    x_norm1 = layers.LayerNormalization()(x)\n",
    "    attn = layers.MultiHeadAttention(\n",
    "        num_heads=heads,\n",
    "        key_dim=dim_head,\n",
    "        dropout=dropout,\n",
    "    )(x_norm1, x_norm1)\n",
    "    attn = layers.Dense(dim)(attn)\n",
    "    attn = layers.Dropout(dropout)(attn)\n",
    "    x = layers.Add()([x, attn])\n",
    "\n",
    "    # --- Feed-forward (PreNorm) ---\n",
    "    x_norm2 = layers.LayerNormalization()(x)\n",
    "    ff = layers.Dense(mlp_dim, activation=\"gelu\")(x_norm2)\n",
    "    ff = layers.Dropout(dropout)(ff)\n",
    "    ff = layers.Dense(dim)(ff)\n",
    "    ff = layers.Dropout(dropout)(ff)\n",
    "    x = layers.Add()([x, ff])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CLS TOKEN + POSITIONAL EMBEDDING\n",
    "# ============================================================\n",
    "\n",
    "class AddClassTokenAndPosEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patches, dim, emb_dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.dim = dim\n",
    "        self.dropout = layers.Dropout(emb_dropout)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.pos_embedding = self.add_weight(\n",
    "            name=\"pos_embedding\",\n",
    "            shape=(1, self.num_patches + 1, self.dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.cls_token = self.add_weight(\n",
    "            name=\"cls_token\",\n",
    "            shape=(1, 1, self.dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        B = tf.shape(x)[0]\n",
    "        cls_tokens = tf.repeat(self.cls_token, repeats=B, axis=0)\n",
    "        x = tf.concat([cls_tokens, x], axis=1)\n",
    "        x = x + self.pos_embedding[:, :tf.shape(x)[1], :]\n",
    "        return self.dropout(x, training=training)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EEGViT MODEL (MATCHES TRAINING EXACTLY)\n",
    "# ============================================================\n",
    "\n",
    "def build_eeg_vit_keras(\n",
    "    num_chan=127,\n",
    "    num_time=100,\n",
    "    num_patches=10,\n",
    "    num_classes=1,\n",
    "    dim=32,\n",
    "    depth=4,\n",
    "    heads=16,\n",
    "    mlp_dim=64,\n",
    "    dim_head=64,\n",
    "    dropout=0.1,\n",
    "    emb_dropout=0.1,\n",
    "    pool=\"cls\",\n",
    "):\n",
    "\n",
    "    assert num_time % num_patches == 0\n",
    "    patch_len = num_time // num_patches\n",
    "\n",
    "    inp = layers.Input(shape=(num_chan, num_time), name=\"eeg_input\")\n",
    "\n",
    "    # ---- Patch embedding ----\n",
    "    def to_patches(t):\n",
    "        B = tf.shape(t)[0]\n",
    "        C = tf.shape(t)[1]\n",
    "        t = tf.reshape(t, (B, C, num_patches, patch_len))\n",
    "        t = tf.transpose(t, perm=[0, 2, 1, 3])\n",
    "        t = tf.reshape(t, (B, num_patches, C * patch_len))\n",
    "        return t\n",
    "\n",
    "    x = layers.Lambda(to_patches, name=\"to_patches\")(inp)\n",
    "    x = layers.Dense(dim, name=\"patch_linear\")(x)\n",
    "\n",
    "    # ---- CLS + positional embedding ----\n",
    "    x = AddClassTokenAndPosEmbedding(\n",
    "        num_patches=num_patches,\n",
    "        dim=dim,\n",
    "        emb_dropout=emb_dropout,\n",
    "        name=\"cls_pos_embedding\"\n",
    "    )(x)\n",
    "\n",
    "    # ---- Transformer encoder ----\n",
    "    for _ in range(depth):\n",
    "        x = transformer_block(\n",
    "            x,\n",
    "            dim=dim,\n",
    "            heads=heads,\n",
    "            dim_head=dim_head,\n",
    "            mlp_dim=mlp_dim,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    # ---- Pooling ----\n",
    "    if pool == \"mean\":\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "    else:\n",
    "        x = x[:, 0, :]\n",
    "\n",
    "    x = layers.LayerNormalization(name=\"final_norm\")(x)\n",
    "\n",
    "    out = layers.Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n",
    "    return Model(inp, out, name=\"EEGViT\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXTRACT WEIGHTS FROM .KERAS\n",
    "# ============================================================\n",
    "\n",
    "def extract_weights_from_keras(keras_path):\n",
    "    tmp_dir = tempfile.mkdtemp(prefix=\"keras_weights_\")\n",
    "    with zipfile.ZipFile(keras_path, \"r\") as z:\n",
    "        names = z.namelist()\n",
    "        weight_files = [\n",
    "            n for n in names\n",
    "            if n.endswith(\".h5\") and \"weights\" in n.lower()\n",
    "        ]\n",
    "        if not weight_files:\n",
    "            raise FileNotFoundError(\"No weights file found inside .keras\")\n",
    "        z.extract(weight_files[0], tmp_dir)\n",
    "        return os.path.join(tmp_dir, weight_files[0])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# BUILD MODEL + LOAD WEIGHTS\n",
    "# ============================================================\n",
    "\n",
    "model = build_eeg_vit_keras()\n",
    "\n",
    "weights_path = extract_weights_from_keras(MODEL_PATH)\n",
    "model.load_weights(weights_path)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(name=\"accuracy\", threshold=0.5)]\n",
    ")\n",
    "\n",
    "print(\"✅ EEGViT model loaded successfully.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOAD TRAINING MEAN / STD\n",
    "# ============================================================\n",
    "\n",
    "with open(MEAN_PATH, \"rb\") as f:\n",
    "    train_mean = pickle.load(f)\n",
    "\n",
    "with open(STD_PATH, \"rb\") as f:\n",
    "    train_std = pickle.load(f)\n",
    "\n",
    "print(\"✅ Training mean and std loaded.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAVE SAFE MODEL\n",
    "# ============================================================\n",
    "\n",
    "model.save(SAFE_MODEL_PATH)\n",
    "print(f\"✅ Safe EEGViT model saved to:\\n{SAFE_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44d222-4daf-466f-b55f-07ccac26ebb3",
   "metadata": {},
   "source": [
    "CELL 3 — Load ONLY excluded trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4552ccad-4bc8-4f23-aa72-b04edccd2c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ EEG arrays loaded.\n",
      "Negative shape: (31, 127, 150000)\n",
      "Positive shape: (46, 127, 150000)\n",
      "✅ Extracted excluded trials:\n",
      "Excluded negative: (5, 127, 150000)\n",
      "Excluded positive: (5, 127, 150000)\n"
     ]
    }
   ],
   "source": [
    "# Local EEG data paths\n",
    "NEG_PATH = r\"D:\\Dermerzel\\SomnasNest\\Alzheimer\\Data\\ad_negative.npy\"\n",
    "POS_PATH = r\"D:\\Dermerzel\\SomnasNest\\Alzheimer\\Data\\ad_positive.npy\"\n",
    "\n",
    "# Load full arrays\n",
    "X_neg_full = np.load(NEG_PATH).astype(np.float32)\n",
    "X_pos_full = np.load(POS_PATH).astype(np.float32)\n",
    "\n",
    "print(\"✅ EEG arrays loaded.\")\n",
    "print(\"Negative shape:\", X_neg_full.shape)\n",
    "print(\"Positive shape:\", X_pos_full.shape)\n",
    "\n",
    "# Extract ONLY last 5 trials (excluded during training)\n",
    "X_neg_excluded = X_neg_full[-5:]\n",
    "X_pos_excluded = X_pos_full[-5:]\n",
    "\n",
    "print(\"✅ Extracted excluded trials:\")\n",
    "print(\"Excluded negative:\", X_neg_excluded.shape)\n",
    "print(\"Excluded positive:\", X_pos_excluded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14208a60-780a-419f-8239-5bdba714d398",
   "metadata": {},
   "source": [
    "CELL 4 — Select ONLY last ad_negative sample (target = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee70b32e-df6b-4001-b9ec-e84ce66475a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Selected last excluded ad_positive trial.\n",
      "Trial shape: (127, 150000)\n",
      "Ground truth label: 1\n"
     ]
    }
   ],
   "source": [
    "# Select ONLY the last excluded positive trial\n",
    "X_test_trial = X_pos_excluded[-5]        # last unseen ad_positive sample\n",
    "y_test_trial = 1                         # ground truth label (positive)\n",
    "\n",
    "print(\"✅ Selected last excluded ad_positive trial.\")\n",
    "print(\"Trial shape:\", X_test_trial.shape)\n",
    "print(\"Ground truth label:\", y_test_trial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc0295f-fe0e-489d-97df-27399a0c364a",
   "metadata": {},
   "source": [
    "* downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6e6702a-c1e9-4bd6-b1c8-88a49c9fd15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 253\n",
      "✅ Downsampled X_test_trial to 500 Hz.\n",
      "Trial shape after downsampling: (127, 75000) float32\n"
     ]
    }
   ],
   "source": [
    "# --- Downsample from 1000 Hz -> 500 Hz (keep output name: X_test_trial) ---\n",
    "sfreq_in = 1000\n",
    "sfreq_out = 500\n",
    "\n",
    "X_test_trial = mne.filter.resample(\n",
    "    X_test_trial.astype(np.float64, copy=False),\n",
    "    down=sfreq_in // sfreq_out,   # 2\n",
    "    npad=\"auto\",\n",
    "    axis=-1,\n",
    "    verbose=True\n",
    ").astype(np.float32, copy=False)\n",
    "\n",
    "print(\"✅ Downsampled X_test_trial to 500 Hz.\")\n",
    "print(\"Trial shape after downsampling:\", X_test_trial.shape, X_test_trial.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1148bf-53c8-4dc9-b4c7-7a619900d547",
   "metadata": {},
   "source": [
    "CELL 5 — Helper + preprocessing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0a5bed4-e578-451a-b8c7-3b69c8269fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing helpers defined.\n"
     ]
    }
   ],
   "source": [
    "# Helper to generate channel names\n",
    "def _names_from_index_mapping(n_channels, index_to_name):\n",
    "    return [f\"EEG{i+1}\" for i in range(n_channels)]\n",
    "\n",
    "# Helper to create MNE Raw object\n",
    "def _make_raw(eeg, sfreq, ch_names):\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=\"eeg\")\n",
    "    return mne.io.RawArray(eeg, info, verbose=False)\n",
    "\n",
    "# Wavelet ICA class\n",
    "class WaveletICA:\n",
    "    def __init__(self, wavelet=\"db4\", level=3, n_components=10):\n",
    "        self.wavelet = wavelet\n",
    "        self.level = level\n",
    "        self.n_components = n_components\n",
    "        self.ica = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        coeffs = pywt.wavedec(X, self.wavelet, level=self.level, axis=1)\n",
    "        A = coeffs[0]\n",
    "        self.ica = FastICA(n_components=min(self.n_components, X.shape[0]), random_state=42)\n",
    "        S = self.ica.fit_transform(A.T)\n",
    "        coeffs[0] = self.ica.inverse_transform(S).T\n",
    "        pywt.waverec(coeffs, self.wavelet, axis=1)\n",
    "\n",
    "    def transform(self, X):\n",
    "        coeffs = pywt.wavedec(X, self.wavelet, level=self.level, axis=1)\n",
    "        A = coeffs[0]\n",
    "        S = self.ica.transform(A.T)\n",
    "        coeffs[0] = self.ica.inverse_transform(S).T\n",
    "        return pywt.waverec(coeffs, self.wavelet, axis=1)\n",
    "\n",
    "print(\"✅ Preprocessing helpers defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecabba65-f176-40ee-9616-ad5b75683307",
   "metadata": {},
   "source": [
    "CELL 6 — Apply preprocessing to unseen test trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e670665-042a-4004-a0c1-e5028b004408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "✅ Preprocessing applied.\n",
      "Cleaned trial shape: (127, 75000)\n"
     ]
    }
   ],
   "source": [
    "# Sampling frequency (same as training)\n",
    "fs = 500.0\n",
    "\n",
    "# Create Raw object\n",
    "raw = _make_raw(X_test_trial, fs, _names_from_index_mapping(X_test_trial.shape[0], None))\n",
    "\n",
    "# Notch filtering\n",
    "raw.notch_filter([50, 100, 150], verbose=False)\n",
    "\n",
    "# High-pass filtering\n",
    "raw.filter(l_freq=0.05, h_freq=None, verbose=False)\n",
    "\n",
    "# Common average reference\n",
    "raw.set_eeg_reference(\"average\", projection=True)\n",
    "raw.apply_proj()\n",
    "\n",
    "# Extract cleaned signal\n",
    "X_clean = raw.get_data().astype(np.float32)\n",
    "\n",
    "print(\"✅ Preprocessing applied.\")\n",
    "print(\"Cleaned trial shape:\", X_clean.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3880869e-281e-48f6-adde-aeb7969bf9e4",
   "metadata": {},
   "source": [
    "CELL 7 — Segment test trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8840de4-a842-4379-ac3c-3b6385c9981f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trial segmented.\n",
      "Segments shape: (750, 127, 100)\n"
     ]
    }
   ],
   "source": [
    "# Segment size\n",
    "SEGMENT_SIZE = 100\n",
    "\n",
    "# Segment the trial\n",
    "segments = []\n",
    "for i in range(X_clean.shape[1] // SEGMENT_SIZE):\n",
    "    seg = X_clean[:, i*SEGMENT_SIZE:(i+1)*SEGMENT_SIZE]\n",
    "    segments.append(seg)\n",
    "\n",
    "# Convert to array\n",
    "X_test_segments = np.array(segments, dtype=np.float32)\n",
    "\n",
    "print(\"✅ Trial segmented.\")\n",
    "print(\"Segments shape:\", X_test_segments.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a73a256-a43b-42ea-b6de-1ef798c1e7b7",
   "metadata": {},
   "source": [
    "CELL 8 — Reshape + normalize using TRAINING stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94c63994-1590-45d6-90cb-29ab2674e7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Normalization applied using training statistics.\n",
      "Final test shape: (750, 127, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Add channel dimension\n",
    "X_test = X_test_segments[..., np.newaxis]\n",
    "\n",
    "# Normalize using training mean/std\n",
    "X_test_norm = (X_test - train_mean) / train_std\n",
    "\n",
    "print(\"✅ Normalization applied using training statistics.\")\n",
    "print(\"Final test shape:\", X_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2331cf-a7bc-4b88-84a8-a99960ade91e",
   "metadata": {},
   "source": [
    "CELL 9 — Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67b0ffea-89a3-483a-8b2e-d4f1669879f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inference completed for ad_positive trial.\n",
      "First 10 predicted labels: [1 1 1 1 1 1 1 1 1 1]\n",
      "Total segments: 750\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities for each segment\n",
    "y_probs = model.predict(X_test_norm, verbose=0)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_preds = (y_probs >= 0.5).astype(int).flatten()\n",
    "\n",
    "# Ground truth labels (ALL ones because ad_positive)\n",
    "y_true = np.ones_like(y_preds)\n",
    "\n",
    "print(\"✅ Inference completed for ad_positive trial.\")\n",
    "print(\"First 10 predicted labels:\", y_preds[:10])\n",
    "print(\"Total segments:\", len(y_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c815e60-49db-4e87-9b17-38152116265f",
   "metadata": {},
   "source": [
    "CELL 10 — Confusion matrix plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "000085f8-31ca-407c-acbb-62678e563ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Confusion matrix plotted for ad_positive trial.\n"
     ]
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_preds)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Negative\", \"Positive\"],\n",
    "    yticklabels=[\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix — Last Excluded ad_positive Sample\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Confusion matrix plotted for ad_positive trial.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6490f5e3-268a-42af-91a1-501235308314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Saliency (segments):  16%|█▋        | 122/750 [00:27<02:20,  4.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_13276\\2866596788.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Compute saliency for each segment in your unseen trial\u001b[39;00m\n\u001b[32m     59\u001b[39m all_saliencies = []\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;28;01min\u001b[39;00m tqdm(range(X_test_norm.shape[\u001b[32m0\u001b[39m]), desc=\u001b[33m\"Computing Saliency (segments)\"\u001b[39m):\n\u001b[32m     61\u001b[39m     sample = X_test_norm[i:i+\u001b[32m1\u001b[39m]                          \u001b[38;5;66;03m# (1, C, T, 1)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     sal = compute_electrode_saliency(model, sample, class_index=\u001b[32m1\u001b[39m)\n\u001b[32m     63\u001b[39m     all_saliencies.append(sal)\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m all_saliencies = np.array(all_saliencies, dtype=np.float32)   \u001b[38;5;66;03m# (n_segments, C)\u001b[39;00m\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_13276\\2866596788.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(model, input_sample, class_index)\u001b[39m\n\u001b[32m     46\u001b[39m             target = pred[:, \u001b[32m0\u001b[39m]                          \u001b[38;5;66;03m# sigmoid probability\u001b[39;00m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     48\u001b[39m             target = pred[:, class_index]                \u001b[38;5;66;03m# softmax probability for class_index\u001b[39;00m\n\u001b[32m     49\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     grads = tape.gradient(target, input_sample)          \u001b[38;5;66;03m# same shape as input_sample\u001b[39;00m\n\u001b[32m     51\u001b[39m     grads = tf.abs(grads)\n\u001b[32m     52\u001b[39m \n\u001b[32m     53\u001b[39m     \u001b[38;5;66;03m# Average over time and \"channel dimension\" (the last dim=1) -> keep electrodes\u001b[39;00m\n",
      "\u001b[32m~\\anaconda3\\envs\\DL\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[39m\n\u001b[32m   1062\u001b[39m               output_gradients))\n\u001b[32m   1063\u001b[39m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[32m   1064\u001b[39m                           \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m output_gradients]\n\u001b[32m   1065\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m     flat_grad = imperative_grad.imperative_grad(\n\u001b[32m   1067\u001b[39m         self._tape,\n\u001b[32m   1068\u001b[39m         flat_targets,\n\u001b[32m   1069\u001b[39m         flat_sources,\n",
      "\u001b[32m~\\anaconda3\\envs\\DL\\Lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[39m\n\u001b[32m     63\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m ValueError:\n\u001b[32m     64\u001b[39m     raise ValueError(\n\u001b[32m     65\u001b[39m         \u001b[33m\"Unknown value for unconnected_gradients: %r\"\u001b[39m % unconnected_gradients)\n\u001b[32m     66\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[32m     68\u001b[39m       tape._tape,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m     69\u001b[39m       target,\n\u001b[32m     70\u001b[39m       sources,\n",
      "\u001b[32m~\\anaconda3\\envs\\DL\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[39m\n\u001b[32m    144\u001b[39m     gradient_name_scope = \u001b[33m\"gradient_tape/\"\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m forward_pass_name_scope:\n\u001b[32m    146\u001b[39m       gradient_name_scope += forward_pass_name_scope + \u001b[33m\"/\"\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(gradient_name_scope):\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, *out_grads)\n\u001b[32m    149\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, *out_grads)\n",
      "\u001b[32m~\\anaconda3\\envs\\DL\\Lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(op, grad)\u001b[39m\n\u001b[32m   1431\u001b[39m     gy = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1432\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1433\u001b[39m     gy = gen_math_ops.mul(math_ops.conj(x), grad)\n\u001b[32m   1434\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1435\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m _ReduceGradientArgs(x, y, gx, gy)\n",
      "\u001b[32m~\\anaconda3\\envs\\DL\\Lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, gx, gy)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m _ReduceGradientArgs(x, y, gx, gy):\n\u001b[32m    140\u001b[39m   \u001b[33m\"\"\"Reduces gradients of both arguments of a broadcasting binary op.\"\"\"\u001b[39m\n\u001b[32m    141\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m gx \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mor\u001b[39;00m gy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    142\u001b[39m     bx, by = SmartBroadcastGradientArgs(x, y)\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     gx = _ReduceGradientArg(gx, bx)\n\u001b[32m    144\u001b[39m     gy = _ReduceGradientArg(gy, by)\n\u001b[32m    145\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m gx, gy\n",
      "\u001b[32m~\\anaconda3\\envs\\DL\\Lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(grad, shape_axes_must_reduce)\u001b[39m\n\u001b[32m    130\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m grad \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m must_reduce:\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m# Applying keepdims=True in presence of unknown axes opens up some\u001b[39;00m\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# opportunities for optimizations. For example, _SumGrad below won't have to\u001b[39;00m\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# emit extra ops to recover reduced indices for broadcasting.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     grad = math_ops.reduce_sum(grad, axes, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    135\u001b[39m     grad = array_ops.reshape(grad, shape)\n\u001b[32m    136\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m grad\n",
      "\u001b[32m~\\anaconda3\\envs\\DL\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m op(*args, **kwargs)\n\u001b[32m     89\u001b[39m     bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m     90\u001b[39m     bound_arguments.apply_defaults()\n\u001b[32m     91\u001b[39m     bound_kwargs = bound_arguments.arguments\n",
      "\u001b[32m~\\anaconda3\\envs\\DL\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m~\\anaconda3\\envs\\DL\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1261\u001b[39m \n\u001b[32m   1262\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1263\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1264\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1265\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1266\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1267\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1268\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32m~\\anaconda3\\envs\\DL\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(input_tensor, axis, keepdims, name)\u001b[39m\n\u001b[32m   2254\u001b[39m   int64 \u001b[38;5;28;01mwhile\u001b[39;00m tensorflow returns the same dtype \u001b[38;5;28;01mas\u001b[39;00m the input.\n\u001b[32m   2255\u001b[39m   @end_compatibility\n\u001b[32m   2256\u001b[39m   \"\"\"\n\u001b[32m   2257\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m2258\u001b[39m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[32m   2259\u001b[39m                               _ReductionDims(input_tensor, axis))\n",
      "\u001b[32m~\\anaconda3\\envs\\DL\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(input_tensor, axis, keepdims, name, dims)\u001b[39m\n\u001b[32m   2266\u001b[39m                          dims=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2267\u001b[39m   keepdims = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m bool(keepdims)\n\u001b[32m   2268\u001b[39m   return _may_reduce_to_scalar(\n\u001b[32m   2269\u001b[39m       keepdims, axis,\n\u001b[32m-> \u001b[39m\u001b[32m2270\u001b[39m       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\n",
      "\u001b[32m~\\anaconda3\\envs\\DL\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(input, axis, keep_dims, name)\u001b[39m\n\u001b[32m  12373\u001b[39m         _ctx, \u001b[33m\"Sum\"\u001b[39m, name, input, axis, \u001b[33m\"keep_dims\"\u001b[39m, keep_dims)\n\u001b[32m  12374\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m  12375\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m  12376\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m> \u001b[39m\u001b[32m12377\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m  12378\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m  12379\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m  12380\u001b[39m       return _sum_eager_fallback(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# %% Step 11 - Gradient-based saliency scores (for your unseen trial segments)\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# ✅ Your electrode list (must match channel order in X_test_norm)\n",
    "electrode_names = [\n",
    "    'Fp1', 'Fz', 'F3', 'F7', 'FT9', 'FC5', 'FC1', 'C3', 'T7', 'TP9', 'CP5', 'CP1', 'Pz', 'P3', 'P7', 'O1', 'Oz', 'O2',\n",
    "    'P4', 'P8', 'TP10', 'CP6', 'CP2', 'Cz', 'C4', 'T8', 'FT10', 'FC6', 'FC2', 'F4', 'F8', 'Fp2', 'AF7', 'AF3', 'AFz',\n",
    "    'F1', 'F5', 'FT7', 'FC3', 'C1', 'C5', 'TP7', 'CP3', 'P1', 'P5', 'PO7', 'PO3', 'POz', 'PO4', 'PO8', 'P6', 'P2',\n",
    "    'CPz', 'CP4', 'TP8', 'C6', 'C2', 'FC4', 'FT8', 'F6', 'AF8', 'AF4', 'F2', 'F9', 'AFF1h', 'FFC1h', 'FFC5h', 'FTT7h',\n",
    "    'FCC3h', 'CCP1h', 'CCP5h', 'TPP7h', 'P9', 'PPO9h', 'PO9', 'O9', 'OI1h', 'PPO1h', 'CPP3h', 'CPP4h', 'PPO2h', 'OI2h',\n",
    "    'O10', 'PO10', 'PPO10h', 'P10', 'TPP8h', 'CCP6h', 'CCP2h', 'FCC4h', 'FTT8h', 'FFC6h', 'FFC2h', 'AFF2h', 'F10',\n",
    "    'AFp1', 'AFF5h', 'FFT9h', 'FFT7h', 'FFC3h', 'FCC1h', 'FCC5h', 'FTT9h', 'TTP7h', 'CCP3h', 'CPP1h', 'CPP5h', 'TPP9h',\n",
    "    'POO9h', 'PPO5h', 'POO1', 'POO2', 'PPO6h', 'POO10h', 'TPP10h', 'CPP6h', 'CPP2h', 'CCP4h', 'TTP8h', 'FTT10h',\n",
    "    'FCC6h', 'FCC2h', 'FFC4h', 'FFT8h', 'FFT10h', 'AFF6h', 'AFp2'\n",
    "]\n",
    "\n",
    "# --- Safety check: electrode count must match your data channel count ---\n",
    "n_channels_data = X_test_norm.shape[1]\n",
    "if len(electrode_names) != n_channels_data:\n",
    "    print(f\"⚠️ Electrode list length = {len(electrode_names)} but X_test_norm has {n_channels_data} channels.\")\n",
    "    # Fallback: auto-generate names to avoid crashing (or trim if electrode list is longer)\n",
    "    if len(electrode_names) > n_channels_data:\n",
    "        electrode_names = electrode_names[:n_channels_data]\n",
    "        print(f\"✅ Trimmed electrode list to {n_channels_data}.\")\n",
    "    else:\n",
    "        electrode_names = [f\"EEG{i+1}\" for i in range(n_channels_data)]\n",
    "        print(f\"✅ Using auto-generated names EEG1..EEG{n_channels_data}.\")\n",
    "\n",
    "def compute_electrode_saliency(model, input_sample, class_index=1):\n",
    "    \"\"\"\n",
    "    Compute per-electrode saliency for ONE sample shaped like (1, C, T, 1).\n",
    "    Works for sigmoid (1 output) or softmax (>=2 outputs).\n",
    "    \"\"\"\n",
    "    input_sample = tf.convert_to_tensor(input_sample, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_sample)\n",
    "        pred = model(input_sample, training=False)\n",
    "\n",
    "        # Make a scalar target to differentiate\n",
    "        if pred.shape[-1] == 1:\n",
    "            target = pred[:, 0]                          # sigmoid probability\n",
    "        else:\n",
    "            target = pred[:, class_index]                # softmax probability for class_index\n",
    "\n",
    "    grads = tape.gradient(target, input_sample)          # same shape as input_sample\n",
    "    grads = tf.abs(grads)\n",
    "\n",
    "    # Average over time and \"channel dimension\" (the last dim=1) -> keep electrodes\n",
    "    # input shape: (1, C, T, 1) => reduce over T and last dim\n",
    "    saliency = tf.reduce_mean(grads, axis=(2, 3))        # -> (1, C)\n",
    "    return saliency.numpy().flatten()\n",
    "\n",
    "# Compute saliency for each segment in your unseen trial\n",
    "all_saliencies = []\n",
    "for i in tqdm(range(X_test_norm.shape[0]), desc=\"Computing Saliency (segments)\"):\n",
    "    sample = X_test_norm[i:i+1]                          # (1, C, T, 1)\n",
    "    sal = compute_electrode_saliency(model, sample, class_index=1)\n",
    "    all_saliencies.append(sal)\n",
    "\n",
    "all_saliencies = np.array(all_saliencies, dtype=np.float32)   # (n_segments, C)\n",
    "\n",
    "# Aggregate across segments\n",
    "mean_saliency = np.mean(all_saliencies, axis=0)               # (C,)\n",
    "normalized_saliency = mean_saliency / (np.max(mean_saliency) + 1e-8)\n",
    "\n",
    "# Save CSV\n",
    "saliency_df = pd.DataFrame({\n",
    "    \"Electrode\": electrode_names,\n",
    "    \"Saliency Score\": normalized_saliency\n",
    "})\n",
    "\n",
    "out_csv = \"gradient_saliency_scores_last_excluded_ad_positive_trial.csv\"\n",
    "saliency_df.to_csv(out_csv, index=False)\n",
    "print(f\"✅ Saved gradient-based saliency scores to '{out_csv}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081642ef-8230-4e7e-9944-7ab06b9a7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [NEW CELL] Clear topomap: BIG dots + labels with leader lines (robust for your channel set)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "\n",
    "# -------- 1) Load values --------\n",
    "if \"normalized_saliency\" in globals() and \"electrode_names\" in globals():\n",
    "    ch_names_orig = list(electrode_names)\n",
    "    values_orig = np.asarray(normalized_saliency, dtype=float)\n",
    "else:\n",
    "    # Change filename if needed\n",
    "    df = pd.read_csv(\"70th_gradient_saliency_scores.csv\")\n",
    "    ch_names_orig = df[\"Electrode\"].astype(str).tolist()\n",
    "    values_orig = df[\"Saliency Score\"].to_numpy(dtype=float)\n",
    "\n",
    "# -------- 2) Build mapping to montage channel names --------\n",
    "# Many high-density names end with \"h\" (e.g., AFF1h). Standard montages often store them without \"h\".\n",
    "def montage_lookup_name(name: str) -> str:\n",
    "    n = name.strip()\n",
    "    if n.endswith(\"h\") and len(n) > 1:\n",
    "        return n[:-1]  # AFF1h -> AFF1\n",
    "    return n\n",
    "\n",
    "ch_names_lookup = [montage_lookup_name(n) for n in ch_names_orig]\n",
    "\n",
    "# -------- 3) Create info & set a montage with custom positions --------\n",
    "# Use standard_1005 for broader coverage than standard_1020\n",
    "base_montage = mne.channels.make_standard_montage(\"standard_1005\")\n",
    "base_pos = base_montage.get_positions()[\"ch_pos\"]  # dict: {name: xyz}\n",
    "\n",
    "# Create a custom position dict for *your original* channel names\n",
    "# by borrowing xyz from the lookup name if available.\n",
    "ch_pos_custom = {}\n",
    "keep_idx = []\n",
    "for i, (orig, look) in enumerate(zip(ch_names_orig, ch_names_lookup)):\n",
    "    if look in base_pos:\n",
    "        ch_pos_custom[orig] = base_pos[look]\n",
    "        keep_idx.append(i)\n",
    "\n",
    "if len(keep_idx) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"No channels matched the montage. Try a different montage (e.g., 'biosemi160') \"\n",
    "        \"or provide a custom electrode position file.\"\n",
    "    )\n",
    "\n",
    "# Keep only channels we can place\n",
    "ch_names = [ch_names_orig[i] for i in keep_idx]\n",
    "values = values_orig[keep_idx]\n",
    "\n",
    "# Create info and apply custom dig montage\n",
    "info = mne.create_info(ch_names=ch_names, sfreq=1000.0, ch_types=\"eeg\")\n",
    "custom_montage = mne.channels.make_dig_montage(ch_pos=ch_pos_custom, coord_frame=\"head\")\n",
    "info.set_montage(custom_montage, on_missing=\"ignore\")\n",
    "\n",
    "# -------- 4) Compute 2D topomap coordinates (only valid channels exist here) --------\n",
    "coords2d = mne.channels.layout._find_topomap_coords(info, picks=np.arange(len(ch_names)))\n",
    "\n",
    "# -------- 5) Plot BIG + crisp --------\n",
    "fig, ax = plt.subplots(figsize=(14, 9), dpi=250)\n",
    "\n",
    "vmin, vmax = float(values.min()), float(values.max())  # or set fixed range manually\n",
    "\n",
    "im, cn = mne.viz.plot_topomap(\n",
    "    values,\n",
    "    info,\n",
    "    axes=ax,\n",
    "    show=False,\n",
    "    contours=6,\n",
    "    sensors=False,      # draw our own dots\n",
    "    outlines=\"head\",\n",
    "    vlim=(vmin, vmax),\n",
    "    cmap=\"jet\",\n",
    ")\n",
    "\n",
    "# -------- 6) Draw big dots + labels with outward leader lines --------\n",
    "dot_size = 18\n",
    "font_size = 13\n",
    "arrow_lw = 0.9\n",
    "\n",
    "for (x, y), name in zip(coords2d, ch_names):\n",
    "    # big dot\n",
    "    ax.plot(x, y, \"k.\", markersize=dot_size, zorder=10)\n",
    "\n",
    "    # radial outward label placement\n",
    "    r = np.sqrt(x * x + y * y) + 1e-9\n",
    "    ux, uy = x / r, y / r  # outward unit vector\n",
    "\n",
    "    # base offset outward + a tiny vertical lift to reduce overlap\n",
    "    offset_out = 0.085\n",
    "    offset_lift = 0.015\n",
    "\n",
    "    x_text = x + ux * offset_out\n",
    "    y_text = y + uy * offset_out + (offset_lift if y >= 0 else -offset_lift)\n",
    "\n",
    "    ax.annotate(\n",
    "        name,\n",
    "        xy=(x, y),\n",
    "        xytext=(x_text, y_text),\n",
    "        textcoords=\"data\",\n",
    "        fontsize=font_size,\n",
    "        fontweight=\"bold\",\n",
    "        ha=\"left\" if x_text >= x else \"right\",\n",
    "        va=\"center\",\n",
    "        bbox=dict(facecolor=\"white\", edgecolor=\"black\", alpha=0.88, pad=0.25),\n",
    "        arrowprops=dict(arrowstyle=\"-\", color=\"black\", lw=arrow_lw),\n",
    "        zorder=11,\n",
    "    )\n",
    "\n",
    "# -------- 7) Colorbar + title --------\n",
    "cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label(\"Electrode importance\", fontsize=14)\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "missing = len(ch_names_orig) - len(ch_names)\n",
    "ax.set_title(\n",
    "    f\"Topographic map of electrode importance (clear labels)\\n\"\n",
    "    f\"Plotted {len(ch_names)} channels (skipped {missing} without montage positions)\",\n",
    "    fontsize=18,\n",
    ")\n",
    "\n",
    "ax.set_axis_off()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a464ca7-fd1a-4302-a313-134d4d7fcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% CELL 9B — Calculate Average Probability (Production-Style)\n",
    "\n",
    "\"\"\"\n",
    "This matches the production code calculation:\n",
    "1. Get sigmoid probabilities per segment\n",
    "2. Average across all segments\n",
    "3. Determine class and confidence\n",
    "\"\"\"\n",
    "\n",
    "# Get raw probabilities (already computed in Cell 9)\n",
    "# y_probs is shape (n_segments, 1) - sigmoid output per segment\n",
    "\n",
    "# Calculate average probability across all segments\n",
    "avg_probability = float(np.mean(y_probs))\n",
    "\n",
    "# Determine predicted class using 0.5 threshold\n",
    "predicted_class = \"positive\" if avg_probability >= 0.5 else \"negative\"\n",
    "\n",
    "# Calculate confidence (production logic)\n",
    "if avg_probability >= 0.5:\n",
    "    confidence = avg_probability\n",
    "else:\n",
    "    confidence = 1.0 - avg_probability\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*60)\n",
    "print(\"PRODUCTION-STYLE PROBABILITY CALCULATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total segments analyzed: {len(y_probs)}\")\n",
    "print(f\"Average sigmoid probability: {avg_probability:.6f}\")\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Confidence: {confidence:.3f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Breakdown statistics\n",
    "print(\"\\nSegment Probability Statistics:\")\n",
    "print(f\"  Min probability: {y_probs.min():.6f}\")\n",
    "print(f\"  Max probability: {y_probs.max():.6f}\")\n",
    "print(f\"  Median probability: {np.median(y_probs):.6f}\")\n",
    "print(f\"  Std dev: {y_probs.std():.6f}\")\n",
    "\n",
    "# Distribution of predictions\n",
    "num_positive_segments = (y_probs >= 0.5).sum()\n",
    "num_negative_segments = (y_probs < 0.5).sum()\n",
    "print(f\"\\nSegment-level predictions:\")\n",
    "print(f\"  Positive segments: {num_positive_segments} ({100*num_positive_segments/len(y_probs):.1f}%)\")\n",
    "print(f\"  Negative segments: {num_negative_segments} ({100*num_negative_segments/len(y_probs):.1f}%)\")\n",
    "\n",
    "# Expected output format (matches production)\n",
    "output_json = {\n",
    "    \"status\": predicted_class,\n",
    "    \"confidence\": round(confidence, 3),\n",
    "    \"probability\": round(avg_probability, 3),\n",
    "    \"metadata\": {\n",
    "        \"total_segments\": len(y_probs),\n",
    "        \"positive_segments\": int(num_positive_segments),\n",
    "        \"negative_segments\": int(num_negative_segments),\n",
    "        \"ground_truth\": \"positive\" if y_test_trial == 1 else \"negative\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRODUCTION OUTPUT FORMAT:\")\n",
    "print(\"=\"*60)\n",
    "import json\n",
    "print(json.dumps(output_json, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03be33e-0842-4f11-9342-e61778523f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
